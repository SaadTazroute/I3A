{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "cVJjtG3M5O9O",
    "outputId": "6c93ddbc-b8f1-4e83-98d9-59153035e9f5"
   },
   "outputs": [],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAiCT1Oi6iG_"
   },
   "source": [
    "## Téléchargement et format des données\n",
    "\n",
    "Nous avons un fichier a 7 colonnes\n",
    "\n",
    "MOTS ---- POS ---- HEAD_ID ---- DEPREL ---- TARGET_OR_NOT ---- LABEL_ENCODE_1 ---- LABEL_ENCODE_2\n",
    "\n",
    "---\n",
    "Les 4 premières vous les connaissez déjà, elles ont été predites automatiquement avec un analyzeur syntaxique (UDPipe)\n",
    "\n",
    "Les 3 dernières sont des colonnes associées à la semantique en frames (FrameNet)\n",
    "\n",
    "**TARGET_OR_NOT**: indique si le mot peut déclancher une frame (T) ou pas (N)\n",
    "\n",
    "**LABEL_ENCODE_1**: une façon de encoder les labels semantiques     [BIO] + [FE ou TG] + [ROLE or FRAME]\n",
    "\n",
    "**LABEL_ENCODE_2**: une deuxième façon de encoder les labels semantiques     [BIO] + FRAME_NAME + [FE ou TG] + [ROLE or LEXICAL_UNIT]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Dans ce fichier il y a déjà eu un pretraitement pour selectioner quels sont les TARGET possibles. Si une phrase a 2 TARGETS ou plus elle a été dupliqué autant de fois qu'il y avait de TARGETS, pour pouvoir traiter un TARGET à la fois.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "9W8DgSn35VmW",
    "outputId": "8869d20e-20bb-4725-fc0e-eaa70da6fa3d"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f frames.data ]; then\n",
    "    wget -q https://pageperso.lis-lab.fr/gabriel.marzinotto/frames.data\n",
    "fi\n",
    "head -20 frames.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPe_M7zQ9JPU"
   },
   "source": [
    "La première chose à faire est charger les données. Un premier tableau contiendra les mots de chaque phrase, et un second les POS, puis un pour TARGET_OR_NOT et pous de tableaux pour les étiquettes encodées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "JQ1mDg0F9EcB",
    "outputId": "68e13565-0af2-4bc7-9b08-34454523cd60"
   },
   "outputs": [],
   "source": [
    "list_text = []\n",
    "list_pos = []\n",
    "list_tg_or_not = []\n",
    "list_labels_1 = []\n",
    "list_labels_2 = []\n",
    "\n",
    "with open('frames.data') as fp:\n",
    "    text = []\n",
    "    pos = []\n",
    "    tg_tag = []\n",
    "    label_1 = []\n",
    "    label_2 = []\n",
    "    \n",
    "    for line in fp:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        #print(tokens)\n",
    "        if len(tokens) > 1:\n",
    "            text.append(tokens[0])\n",
    "            pos.append(tokens[1])\n",
    "            tg_tag.append(tokens[4])\n",
    "            label_1.append(tokens[5])\n",
    "            label_2.append(tokens[6])\n",
    "\n",
    "        else:\n",
    "            list_text.append(text)\n",
    "            list_pos.append(pos)\n",
    "            list_tg_or_not.append(tg_tag)\n",
    "            list_labels_1.append(label_1)\n",
    "            list_labels_2.append(label_2)\n",
    "\n",
    "            text = []\n",
    "            pos = []\n",
    "            tg_tag = []\n",
    "            label_1 = []\n",
    "            label_2 = []\n",
    "\n",
    "print(len(list_text))\n",
    "\n",
    "print(list_text[12])\n",
    "print(list_labels_1[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1FnJ8Xi_0Ip"
   },
   "source": [
    "Il faut ensuite convertir les étiquettes et les mots en entiers. Nous allons devoir créer des séquences de taille fixe, donc il faut réserver une étiquette de padding, ici `<eos>`. Toutefois, pour l'instant nous nous contentons de convertir les étiquettes sans appliquer le padding. Un defaultdict est utilisé pour créer le vocabulaire des étiquettes. À chaque fois qu'une nouvelle étiquette est rencontrée, il l'associe à une nouvelle valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "f5yv9IyU_KFA",
    "outputId": "5a3b3858-32a3-40e2-8e1e-d367833d1e27"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<eos>'] = 0\n",
    "\n",
    "int_labels = []\n",
    "for label in list_labels_1:\n",
    "    int_labels.append([label_vocab[token] for token in label])\n",
    "\n",
    "print(int_labels[12])\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoE8LrfcAH_7"
   },
   "source": [
    "Il en va de même pour les textes à qui nous allons dédier un vocabulaire différent de celui des étiquettes. Les embeddings préentraînés seront chargés à partir de ce vocabulaire. Il faut veiller à mettre les mots en minuscules car le vocabulaire des embeddings fasttext est en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6x7qzHAVAJI9",
    "outputId": "edadae54-5104-44b2-c1e4-bc168c92e9af"
   },
   "outputs": [],
   "source": [
    "vocab = collections.defaultdict(lambda: len(vocab))\n",
    "vocab['<eos>'] = 0\n",
    "\n",
    "int_texts = []\n",
    "for text in list_text:\n",
    "    int_texts.append([vocab[token.lower()] for token in text])\n",
    "\n",
    "print(int_texts[12])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgggTg_aCoNt"
   },
   "source": [
    "Egalement nous avons besoin de un input pour indiquer quel est la TARGET. Autrement, s'il y a plusieurs targets dans une phrase, le modèle ne pourrais pas faire la distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "35MXDk5BC6Xk",
    "outputId": "e1e1f46d-0400-4e48-f99d-064003b7af43"
   },
   "outputs": [],
   "source": [
    "tg_indicator_vocab = collections.defaultdict(lambda: len(tg_indicator_vocab))\n",
    "tg_indicator_vocab['<eos>'] = 0\n",
    "\n",
    "int_tg_indicator = []\n",
    "for text in list_tg_or_not:\n",
    "    int_tg_indicator.append([tg_indicator_vocab[token] for token in text])\n",
    "\n",
    "print(int_tg_indicator[12])\n",
    "len(tg_indicator_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dd0sbSOYAbzO"
   },
   "source": [
    "Nous pouvons maintenant créer des vocabulaires inversés pour pouvoir revenir vers les étiquettes et mots. Celà et utile pour vérifier le contenu d'un tenseur, et surtout pour convertir les décisions du système en étiquettes textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrkoyCKVAfVv"
   },
   "outputs": [],
   "source": [
    "rev_label_vocab = {y: x for x, y in label_vocab.items()}\n",
    "rev_vocab = {y: x for x, y in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAyKpJDSAr6L"
   },
   "source": [
    "Nous pouvons maintenant calculer la longueur moyenne de nos exemples de apprentissage.\n",
    "Ceci nous permettra définir une max_length pertinante pour apprendre notre resseaux\n",
    "(Pas trop de padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "lh6y1xUUBaF5",
    "outputId": "a9ca9fdb-ae58-4eec-a9ad-b33703a0331b"
   },
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "GqCVaBkoBB2L",
    "outputId": "d40e78d7-7e78-4eed-9eac-b6e8b18462bc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lengths = []\n",
    "len_texts = [ len(text) for text in list_text ]\n",
    "\n",
    "plt.hist(len_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwlE_sKpAmt3"
   },
   "source": [
    "Importons les modules habituels de pytorch. Les constantes suivantes sont définies :\n",
    "\n",
    "    max_len est la longueur maximale d'une phrase en apprentissage\n",
    "    batch_size est la taille des batches\n",
    "    embed_size est la taille des embeddings préentraînés (nous utiliserons les embeddings téléchargeables sur le site de fasttext qui sont de taille 300.\n",
    "    hidden_size est la taille de l'état caché du RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_1d6ab1Aok_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_len = 50\n",
    "batch_size = 64\n",
    "embed_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcXbWU5EB6Xm"
   },
   "source": [
    "Il faut maintenant créer des tenseurs pytorch avec les données phrases et les étiquettes associées. X et Y sont des tenseurs d'entiers de taille (le nombre d'exemples du corpus, la longueur maximale d'une phrase). Ils sont initialisés à zéro car c'est la valeur de padding.\n",
    "\n",
    "Pour chaque phrase et séquence d'étiquettes associées, nous calculons la longueur effective à intégrer en fonction de la longueur maximale (les phrases et séquences d'étiquettes trop longues sont coupées). Puis nous pouvons les intégrer dans les tenseurs en début de ligne, les fins de lignes étant remplies de zéros (`<eos>`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "t3860ZZsB9_F",
    "outputId": "972647c4-5e9c-49db-f0fd-00d45876de27"
   },
   "outputs": [],
   "source": [
    "X = torch.zeros(len(int_texts), max_len).long()\n",
    "T = torch.zeros(len(int_texts), max_len).long()\n",
    "Y = torch.zeros(len(int_labels), max_len).long()\n",
    "\n",
    "for i, (text, tg, label) in enumerate(zip(int_texts, int_tg_indicator, int_labels)):\n",
    "    length = min(max_len, len(text))\n",
    "    X[i,:length] = torch.LongTensor(text[:length])\n",
    "    T[i,:length] = torch.LongTensor(tg[:length])\n",
    "    Y[i,:length] = torch.LongTensor(label[:length])\n",
    "\n",
    "print(X[12])\n",
    "print(T[12])\n",
    "print(Y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKGbpAoCD1cX"
   },
   "source": [
    "Pour vérifier les performances du système, nous le séparons en un ensemble d'entraînement et un ensemble de développement. Comme il y a 7605 exemple, nous utiliserons les 6000 premiers pour l'entraînement et les 1605 suivants pour la validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qrul-szD2l_"
   },
   "outputs": [],
   "source": [
    "X_train = X[:6000]\n",
    "T_train = T[:6000]\n",
    "Y_train = Y[:6000]\n",
    "\n",
    "X_valid = X[6000:]\n",
    "T_valid = T[6000:]\n",
    "Y_valid = Y[6000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1nPzhSsERT9"
   },
   "source": [
    "Torch contient des classes permettant facilement de charger des batches d'exemples déjà mélangés pour l'entraînement. Nous allons en tirer parti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KS59yoxXH7Fo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_set = TensorDataset(X_train, T_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, T_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRT9d8zeINHs"
   },
   "source": [
    "La prochaine étape consiste en le chargement des embeddings préentraînés. Les embeddings fasttext peuvent être téléchargés depuis la page suivante : \n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "Toutefois, ces embeddings couvrant plusieurs millions de mots, le fichier fait plusieurs GiB et une version filtrée par rapport au vocabulaire du corpus est disponible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKicuq4OIXA3"
   },
   "outputs": [],
   "source": [
    "![ -f wiki.fr.vec.small ] || wget -q http://pageperso.lif.univ-mrs.fr/gabriel.marzinotto/wiki.fr.vec.small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T63VYPaoJ99X"
   },
   "source": [
    "Les embeddings ont le format suivant :\n",
    "\n",
    "    chaque ligne décrit l'embedding d'un mot\n",
    "    le mot est sur la première colone, suivi des valeurs du vecteur dans l'espace de 300 dimension.\n",
    "\n",
    "Nous créons donc un tenseur à zéro, puis plaçons l'embedding de chaque mot du vocabulaire rencontré à la bonne ligne dans le tenseur. Les embeddings des mots non couverts dans ce fichier resteront à zéro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cfbXSs4aKOty",
    "outputId": "90267ecd-3918-4577-db61-a47eee08db45"
   },
   "outputs": [],
   "source": [
    "pretrained_weights = torch.zeros(len(vocab), 300)\n",
    "with open('wiki.fr.vec.small') as fp:\n",
    "    for line in fp:\n",
    "        tokens = line.strip().split()\n",
    "        if tokens[0] in vocab:\n",
    "            try:\n",
    "              pretrained_weights[vocab[tokens[0]]] = torch.FloatTensor([float(x) for x in tokens[1:]])\n",
    "            except:\n",
    "              continue\n",
    "                \n",
    "pretrained_weights[12][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhA7mFgWKtZ2"
   },
   "source": [
    "Le modèle est une class qui étend nn.Module. Son constructeur appelle le constructeur de la class mère, puis déclare les différentes couches : une couche d'embeddings, une couche de GRU et une couche de décision. Cette couche de décision sera appliquée à chaque position de la phrase pour prédire l'étiquette associée.\n",
    "\n",
    "Le premier détail important que nous allons traiter est le problème du padding. Pour pouvoir prendre en entrée des batches de phrases de la même taille, nous avons complété ces dernières avec des 0. Le mot d'indice zéro a un embedding et le système peut donc l'utiliser pour apprendre des régularités des données. Si on utilise un RNN bidirectionnel, l'état caché après chaque padding peut aussi contribuer à la représentation créée. Ces deux problème font que le modèle aura un comportement différent si on change la taille des séquences traitées.\n",
    "\n",
    "Une solution est de forcer l'état caché à rester à zéro en présence de padding. Pour celà il faut spécifier le padding_idx de la couche d'emebdding pour la représentation associée au padding soit toujours le vecteur nul. Ensuite, dans le RNN, l'état caché est calculé comme une transformation affine à partir de l'embedding en entrée et de l'état caché précédent. Comme l'état caché initial est à zéro, et que l'embedding du padding est à zéro, si on désactive le bias dans les calculs (en donnant l'option bias=False au GRU), cela va forcer l'état caché à rester à zéro tout au long du padding. Le modèle peut ainsi traiter des séquences de taille arbitraire, même quand il est bidirectionnel.\n",
    "\n",
    "Le deuxième détail est l'initialisation des embeddings. La méthode diffère selon les versions de pytorch, mais la façon présentée ici marche dans la plupart des cas. Nous allons directement modifier le champ weight de la couche d'embeddings et remplacer ce paramètre par nos embeddings préentraînés. requires_grad=False permet de geler la couche d'embeddings et donc de ne pas les modifier lors de l'apprentissage. Cela permettra en prédiction d'utiliser des embeddings fasttext pour les mots que nous n'avons pas observés dans le corpus d'apprentissage, en espérant qu'un mot d'embedding proche s'y trouvait. Si on omet cette option, les embeddings sont fine-tunés pour maximiser les performances du système.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "4GLvRktvJ7zx",
    "outputId": "ba11970d-d532-412f-89de-5c5fc1f34360"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
    "        self.word_embed.weight = nn.Parameter(pretrained_weights, requires_grad=False)\n",
    "        \n",
    "        #Initialize the embeddings for the TARGET_OR_NOT using the identity matrix\n",
    "        tg_ind = len(tg_indicator_vocab)\n",
    "        self.tg_embed = nn.Embedding(tg_ind, tg_ind, padding_idx=tg_indicator_vocab['<eos>'])\n",
    "        self.tg_embed.weight = nn.Parameter(torch.eye(tg_ind), requires_grad=False)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size+tg_ind, hidden_size, bias=False, num_layers=1, dropout=0.3, bidirectional=False, batch_first=True)\n",
    "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        word_embed = self.word_embed(x)\n",
    "        tg_embed = self.tg_embed(t)\n",
    "        \n",
    "        embed = torch.cat((word_embed,tg_embed),-1)\n",
    "        \n",
    "        output, hidden = self.rnn(embed)\n",
    "        return self.decision(output)\n",
    "\n",
    "rnn_model = RNN()\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sJYI8WSEK3en",
    "outputId": "bb1e0779-22cd-4f8e-edbb-cd657283fd06"
   },
   "outputs": [],
   "source": [
    "rnn_model(Variable(X[:2]), Variable(T[:2])).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aqwp_p6Pqmi"
   },
   "source": [
    "La fonction qui calcule les performances d'un modèle doit accomoder la nouvelle forme des données.\n",
    "En particulier, le critère d'entropie croisée n'accepte que des matrices à deux dimensions, donc il faut redimentionner les scores produits pour qu'ils ait la taille `(batch_size * sequence_length, num_labels)` et les références pour quelles aient la taille `(batch_size * sequence_length)`.\n",
    "\n",
    "Ensuite, il faut modifier le max qui calcule les prédictions pour qu'il agisse sur la dernière dimension du tenseur `y_scores`.\n",
    "\n",
    "Et finalement, pour calculer le score d'une séquence, nous devons ignorer le padding. Pour celà une matrice `mask` est crée qui contient 1 pour les éléments non nuls de la matrice contenant les étiquettes et 0 sinon.\n",
    "On peut calculer le nombre de corrects ainsi que le numérateur de la fonction de performance en appliquant le masque.\n",
    "\n",
    "Le loss, pour être comparable au loss de l'entraînement, n'est pas modifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "NLcMFWOLPoXz",
    "outputId": "a5f4666b-c771-422e-adb2-adc03c4f5b44"
   },
   "outputs": [],
   "source": [
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = correct = num_loss = num_perf = 0\n",
    "    for x, t, y in loader:\n",
    "        y_scores = model(Variable(x, volatile=True), Variable(t, volatile=True))\n",
    "        loss = criterion( y_scores.view(y.size(0) * y.size(1), -1), \n",
    "                          Variable(y.view(y.size(0) * y.size(1)), volatile=True) )\n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "        mask = (y != 0)\n",
    "        correct += torch.sum((y_pred.data == y) * mask)\n",
    "        total_loss += loss.data\n",
    "        num_loss += len(y)\n",
    "        num_perf += torch.sum(mask)\n",
    "        \n",
    "    #print(\"df\", correct, num_perf)\n",
    "    return total_loss / num_loss, float(correct) / float(num_perf)\n",
    "\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGQmfx-JQxMI"
   },
   "source": [
    "La fonction d'apprentissage est modifiée en deux endroits. Tout d'abord, pytorch refuse d'entraîner un modèle contenant des paramètres sans gradient. Nous devons donc filtrer la liste des paramètres passés à l'optimiseur pour qu'elle ne contienne pas les embeddings \"gelés\".\n",
    "\n",
    "Ensuite, nous appliquons le même redimenssionnement des scores et des équeittes pour accomoder le critère d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "-yyVjEAFQ3yY",
    "outputId": "03f1e939-1865-40aa-ff15-fd9cb78ded32"
   },
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = num = 0\n",
    "        for x, t, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(Variable(x), Variable(t))\n",
    "            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1)), volatile=True))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num += len(y)\n",
    "        print(epoch, total_loss / num, perf(model, valid_loader))\n",
    "\n",
    "fit(rnn_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIlR7n8_S0XQ"
   },
   "source": [
    "Une fois le modèle entraîné, nous pouvons écrire une fonction qui génère les prédictions pour une phrase. Cette fonction devra :\n",
    "\n",
    "    convertir les mots en entiers (nous n'avons pas prévu de symbole pour les mots inconnus donc ils seront remplacés par du padding)\n",
    "    faire un batch de taille 1 sous la forme d'un tenseur de dimensions (1, longueur de la phrase)\n",
    "    calculer les scores des étiquettes à chaque position\n",
    "    calculer les prédictions correspondantes\n",
    "    et convertir les entiers prédits en text grâce au dictionnaire inversé créé plus haut\n",
    "\n",
    "Notez que cette fonction peut prendre en entrée des phrases plus longues que max_len et que le temps de calcul n'est pas gaspillé par le padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55851
    },
    "colab_type": "code",
    "id": "i_BJTbAtTMap",
    "outputId": "4d0df0af-ca98-433c-ac47-bccf245bdfe1"
   },
   "outputs": [],
   "source": [
    "# we will use rev_label_vocab to go from ids -> labels\n",
    "def fscore(model, loader):\n",
    "  \n",
    "    def build_arg_matrix(y_tensor):\n",
    "        refs = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict()) )\n",
    "        nb_refs = 0.0\n",
    "        \n",
    "        batch_size, length = y_tensor.shape\n",
    "\n",
    "        for i_ in range(batch_size):\n",
    "          y_sent = y_tensor[i_,:]\n",
    "          start_elem = None\n",
    "          end_elem   = None\n",
    "          label_elem = None\n",
    "\n",
    "          print(i_,nb_refs, len(y_tensor))\n",
    "          for j,y in enumerate(y_sent):\n",
    "  \n",
    "            label = rev_label_vocab[y]\n",
    "            # if padding...\n",
    "            if(y == '<eos>'): \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "\n",
    "              start_elem = None\n",
    "              end_elem   = None\n",
    "              label_elem = None\n",
    "            \n",
    "            \n",
    "            elif(label[0] == \"B\"):\n",
    "              \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "                \n",
    "              start_elem = j\n",
    "              end_elem   = j\n",
    "              label_elem = label[2:]\n",
    "              \n",
    "              \n",
    "            elif(label[0] == \"I\"):\n",
    "                if(label[2:] == label_elem):\n",
    "                  end_elem = j\n",
    "                else:\n",
    "                  if( start_elem and end_elem and label_elem):\n",
    "                    refs[i_][start_elem][end_elem] = label_elem   \n",
    "                    nb_refs += 1\n",
    "              \n",
    "                  start_elem = j\n",
    "                  end_elem   = j\n",
    "                  label_elem = label[2:]              \n",
    "                              \n",
    "            elif(label[0] == \"O\"):              \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "\n",
    "              start_elem = None\n",
    "              end_elem   = None\n",
    "              label_elem = None\n",
    "          \n",
    "          \n",
    "          if( start_elem and end_elem and label_elem):\n",
    "            refs[i_][start_elem][end_elem] = label_elem\n",
    "            nb_refs += 1\n",
    "\n",
    "        return refs, nb_refs\n",
    "  \n",
    "  \n",
    "    def compare(y_true, y_pred):\n",
    "        refs, nb_refs = build_arg_matrix(y_true.numpy())\n",
    "        hyps, nb_hyps = build_arg_matrix(y_pred.numpy())\n",
    "        \n",
    "        #print(refs)\n",
    "        #print('-'*50)\n",
    "        #print(hyps)\n",
    "        \n",
    "        nb_ok   = 0.0\n",
    "        counter = 0\n",
    "        for i, dico in refs.items():\n",
    "          for j, subdico in dico.items():\n",
    "            for k, value in subdico.items():   \n",
    "              if( i in hyps and\n",
    "                  j in hyps[i] and \n",
    "                  k in hyps[i][j] and\n",
    "                  hyps[i][j][k] == value ):\n",
    "                nb_ok += 1.0   \n",
    "              \n",
    "        return nb_ok, nb_refs, nb_hyps\n",
    "\n",
    "    nb_ok = nb_ref = nb_hyp = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for x, t, y in loader:\n",
    "        y_scores = model(Variable(x, volatile=True), Variable(t, volatile=True))        \n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "\n",
    "        k, r, h = compare(y, y_pred)\n",
    "        nb_ok  += k\n",
    "        nb_ref += r\n",
    "        nb_hyp += h\n",
    "\n",
    "    print(nb_ok)\n",
    "    print(nb_ref)\n",
    "    print(nb_hyp)\n",
    "    \n",
    "    prec = nb_ok/nb_hyp\n",
    "    recall = nb_ok/nb_ref\n",
    "    f1 = 2*(prec*recall)/(prec+recall)\n",
    "    \n",
    "    return prec, recall, f1\n",
    "\n",
    "fscore(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIx-ntN7d1dN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Frame_Semantic_Tagger.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
