{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons entraîner un étiqueteur de séquences à prédire les groupes nominaux représentant des entités visuelles dans une phrase. Les données sont extraites du corpus Flicker30k entities et le problème est formalisé comme la prédiction d'une étiquette par mot indiquant si une entité commence sur le mot (B), si elle continue (I) ou si on est à l'exterieur d'une entité. Le type de modèle que nous allons concevoir pourrait être appliqué à un taggeur en parties de discours.\n",
    "\n",
    "Nous allons entraîner un RNN à faire ces prédictions et les embeddings de mots seront initialisés avec des embeddings préentrainés avec fasttext. Nous en profiterons pour créer un RNN moins dépendant de la taille des séquences et capable de traiter en test des séquences plus longues que celles qu'il a vu en entraînement.\n",
    "\n",
    "Les données sont sous la forme d'un mot par ligne suivi de son étiquette. Les phrases sont séparées par des lignes vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f f30kE-captions-bio.txt ]; then\n",
    "    wget -q http://pageperso.lif.univ-mrs.fr/~benoit.favre/files/f30kE-captions-bio.txt.gz \n",
    "    gunzip -f f30kE-captions-bio.txt.gz\n",
    "fi\n",
    "head -20 f30kE-captions-bio.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première chose à faire est charger les données. Un premier tableau contiendra les mots de chaque phrase, et un second les étiquettes associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open('f30kE-captions-bio.txt') as fp:\n",
    "    text = []\n",
    "    label = []\n",
    "    for line in fp:\n",
    "        tokens = line.strip().split()\n",
    "        if len(tokens) == 0:\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "            text = []\n",
    "            label = []\n",
    "        else:\n",
    "            text.append(tokens[0])\n",
    "            label.append(tokens[1])\n",
    "\n",
    "print(len(texts))\n",
    "\n",
    "print(texts[12])\n",
    "print(labels[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut ensuite convertir les étiquettes et les mots en entiers.\n",
    "Nous allons devoir créer des séquences de taille fixe, donc il faut réserver une étiquette de padding, ici `<eos>`.\n",
    "Toutefois, pour l'instant nous nous contentons de convertir les étiquettes sans appliquer le padding.\n",
    "Un `defaultdict` est utilisé pour créer le vocabulaire des étiquettes. À chaque fois qu'une nouvelle étiquette est rencontrée, il l'associe à une nouvelle valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<eos>'] = 0\n",
    "\n",
    "int_labels = []\n",
    "for label in labels:\n",
    "    int_labels.append([label_vocab[token] for token in label])\n",
    "\n",
    "print(int_labels[12])\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il en va de même pour les textes à qui nous allons dédier un vocabulaire différent de celui des étiquettes. Les embeddings préentraînés seront chargés à partir de ce vocabulaire. Il faut veiller à mettre les mots en minuscules car le vocabulaire des embeddings fasttext est en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = collections.defaultdict(lambda: len(vocab))\n",
    "vocab['<eos>'] = 0\n",
    "\n",
    "int_texts = []\n",
    "for text in texts:\n",
    "    int_texts.append([vocab[token.lower()] for token in text])\n",
    "\n",
    "print(int_texts[12])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant créer des vocabulaires inversés pour pouvoir revenir vers les étiquettes et mots. Celà et utile pour vérifier le contenu d'un tenseur, et surtout pour convertir les décisions du système en étiquettes textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_label_vocab = {y: x for x, y in label_vocab.items()}\n",
    "rev_vocab = {y: x for x, y in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importons les modules habituels de pytorch. Les constantes suivantes sont définies :\n",
    "- `max_len` est la longueur maximale d'une phrase en apprentissage\n",
    "- `batch_size` est la taille des batches\n",
    "- `embed_size` est la taille des embeddings préentraînés (nous utiliserons les embeddings téléchargeables sur le site de fasttext qui sont de taille 300.\n",
    "- `hidden_size` est la taille de l'état caché du RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_len = 16\n",
    "batch_size = 64\n",
    "embed_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut maintenant créer des tenseurs pytorch avec les données phrases et les étiquettes associées. X et Y sont des tenseurs d'entiers de taille (le nombre d'exemples du corpus, la longueur maximale d'une phrase). Ils sont initialisés à zéro car c'est la valeur de padding.\n",
    "\n",
    "Pour chaque phrase et séquence d'étiquettes associées, nous calculons la longueur effective à intégrer en fonction de la longueur maximale (les phrases et séquences d'étiquettes trop longues sont coupées). Puis nous pouvons les intégrer dans les tenseurs en début de ligne, les fins de lignes étant remplies de zéros (`<eos>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(len(int_texts), max_len).long()\n",
    "Y = torch.zeros(len(int_labels), max_len).long()\n",
    "\n",
    "for i, (text, label) in enumerate(zip(int_texts, int_labels)):\n",
    "    length = min(max_len, len(text))\n",
    "    X[i,:length] = torch.LongTensor(text[:length])\n",
    "    Y[i,:length] = torch.LongTensor(label[:length])\n",
    "\n",
    "print(X[12])\n",
    "print(Y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour vérifier les performances du système, nous le séparons en un ensemble d'entraînement et un ensemble de développement. Comme il y a 5500 exemple, nous utiliserons les 5000 premiers pour l'entraînement et les 500 suivants pour la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:5000]\n",
    "Y_train = Y[:5000]\n",
    "X_valid = X[5000:]\n",
    "Y_valid = Y[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch contient des classes permettant facilement de charger des batches d'exemples déjà mélangés pour l'entraînement. Nous allons en tirer parti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine étape consiste en le chargement des embeddings préentraînés. Les embeddings fasttext peuvent être téléchargés depuis la page suivante : https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "Toutefois, ces embeddings couvrant plusieurs millions de mots, le fichier fait plusieurs GiB et une version filtrée par rapport au vocabulaire du corpus est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -f wiki.en.filtered.vec ] || wget -q http://pageperso.lif.univ-mrs.fr/~benoit.favre/files/wiki.en.filtered.vec.gz -O - | gunzip > wiki.en.filtered.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les embeddings ont le format suivant :\n",
    "- chaque ligne décrit l'embedding d'un mot\n",
    "- le mot est sur la première colone, suivi des valeurs du vecteur dans l'espace de 300 dimension.\n",
    "\n",
    "Nous créons donc un tenseur à zéro, puis plaçons l'embedding de chaque mot du vocabulaire rencontré à la bonne ligne dans le tenseur. Les embeddings des mots non couverts dans ce fichier resteront à zéro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = torch.zeros(len(vocab), 300)\n",
    "with open('wiki.en.filtered.vec') as fp:\n",
    "    for line in fp:\n",
    "        tokens = line.strip().split()\n",
    "        if tokens[0] in vocab:\n",
    "            pretrained_weights[vocab[tokens[0]]] = torch.FloatTensor([float(x) for x in tokens[1:]])\n",
    "\n",
    "pretrained_weights[12][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est une class qui étend `nn.Module`. Son constructeur appelle le constructeur de la class mère, puis déclare les différentes couches : une couche d'embeddings, une couche de GRU et une couche de décision. Cette couche de décision sera appliquée à chaque position de la phrase pour prédire l'étiquette associée.\n",
    "\n",
    "Le premier détail important que nous allons traiter est le problème du padding. Pour pouvoir prendre en entrée des batches de phrases de la même taille, nous avons complété ces dernières avec des 0. Le mot d'indice zéro a un embedding et le système peut donc l'utiliser pour apprendre des régularités des données. Si on utilise un RNN bidirectionnel, l'état caché après chaque padding peut aussi contribuer à la représentation créée. Ces deux problème font que le modèle aura un comportement différent si on change la taille des séquences traitées.\n",
    "\n",
    "Une solution est de forcer l'état caché à rester à zéro en présence de padding. Pour celà il faut spécifier le `padding_idx` de la couche d'emebdding pour la représentation associée au padding soit toujours le vecteur nul. Ensuite, dans le RNN, l'état caché est calculé comme une transformation affine à partir de l'embedding en entrée et de l'état caché précédent. Comme l'état caché initial est à zéro, et que l'embedding du padding est à zéro, si on désactive le bias dans les calculs (en donnant l'option `bias=False` au GRU), cela va forcer l'état caché à rester à zéro tout au long du padding. Le modèle peut ainsi traiter des séquences de taille arbitraire, même quand il est bidirectionnel.\n",
    "\n",
    "Le deuxième détail est l'initialisation des embeddings. La méthode diffère selon les versions de pytorch, mais la façon présentée ici marche dans la plupart des cas. Nous allons directement modifier le champ `weight` de la couche d'embeddings et remplacer ce paramètre par nos embeddings préentraînés. `requires_grad=False` permet de geler la couche d'embeddings et donc de ne pas les modifier lors de l'apprentissage. Cela permettra en prédiction d'utiliser des embeddings fasttext pour les mots que nous n'avons pas observés dans le corpus d'apprentissage, en espérant qu'un mot d'embedding proche s'y trouvait. Si on omet cette option, les embeddings sont fine-tunés pour maximiser les performances du système."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
    "        self.embed.weight = nn.Parameter(pretrained_weights, requires_grad=False)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, bias=False, num_layers=1, dropout=0.3, bidirectional=False, batch_first=True)\n",
    "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x)\n",
    "        output, hidden = self.rnn(embed)\n",
    "        return self.decision(output)\n",
    "\n",
    "rnn_model = RNN()\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de tester le modèle avec un batch créé à partir des exemples du corpus. Nous pouvons remarquer que la taille du tenseur produit est `(batch_size, sequence_length, num_labels)`, il y a bien une prédiction par position dans les séquences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model(Variable(X[:2])).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction qui calcule les performances d'un modèle doit accomoder la nouvelle forme des données.\n",
    "En particulier, le critère d'entropie croisée n'accepte que des matrices à deux dimensions, donc il faut redimentionner les scores produits pour qu'ils ait la taille `(batch_size * sequence_length, num_labels)` et les références pour quelles aient la taille `(batch_size * sequence_length)`.\n",
    "\n",
    "Ensuite, il faut modifier le max qui calcule les prédictions pour qu'il agisse sur la dernière dimension du tenseur `y_scores`.\n",
    "\n",
    "Et finalement, pour calculer le score d'une séquence, nous devons ignorer le padding. Pour celà une matrice `mask` est crée qui contient 1 pour les éléments non nuls de la matrice contenant les étiquettes et 0 sinon.\n",
    "On peut calculer le nombre de corrects ainsi que le numérateur de la fonction de performance en appliquant le masque.\n",
    "\n",
    "Le loss, pour être comparable au loss de l'entraînement, n'est pas modifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = correct = num_loss = num_perf = 0\n",
    "    for x, y in loader:\n",
    "        y_scores = model(Variable(x, volatile=True))\n",
    "        loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1)), volatile=True))\n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "        mask = (y != 0)\n",
    "        correct += torch.sum((y_pred.data == y) * mask)\n",
    "        total_loss += loss.data[0]\n",
    "        num_loss += len(y)\n",
    "        num_perf += torch.sum(mask)\n",
    "    return total_loss / num_loss, correct / num_perf\n",
    "\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction d'apprentissage est modifiée en deux endroits. Tout d'abord, pytorch refuse d'entraîner un modèle contenant des paramètres sans gradient. Nous devons donc filtrer la liste des paramètres passés à l'optimiseur pour qu'elle ne contienne pas les embeddings \"gelés\".\n",
    "\n",
    "Ensuite, nous appliquons le même redimenssionnement des scores et des équeittes pour accomoder le critère d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = num = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(Variable(x))\n",
    "            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1)), volatile=True))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "            num += len(y)\n",
    "        print(epoch, total_loss / num, perf(model, valid_loader))\n",
    "\n",
    "fit(rnn_model, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le modèle entraîné, nous pouvons écrire une fonction qui génère les prédictions pour une phrase.\n",
    "Cette fonction devra :\n",
    "- convertir les mots en entiers (nous n'avons pas prévu de symbole pour les mots inconnus donc ils seront remplacés par du padding)\n",
    "- faire un batch de taille 1 sous la forme d'un tenseur de dimensions (1, longueur de la phrase)\n",
    "- calculer les scores des étiquettes à chaque position\n",
    "- calculer les prédictions correspondantes\n",
    "- et convertir les entiers prédits en text grâce au dictionnaire inversé créé plus haut\n",
    "\n",
    "Notez que cette fonction peut prendre en entrée des phrases plus longues que `max_len` et que le temps de calcul n'est pas gaspillé par le padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice\n",
    "---------\n",
    "\n",
    "Écrire une fonction `predict(sentence)` qui prend en entrée une phrase sous la forme d'une chaîne de caractères et renvoie une liste de couples (étiquette, mot).\n",
    "\n",
    "Pour aller plus loin\n",
    "-----------------------\n",
    "\n",
    "- Calculer le F-score sur les segments biens reconnus dans la fonction d'évaluation des performances sur l'ensemble de validation.\n",
    "- Quel est l'impact des hyper-paramètres sur le modèle ?\n",
    "- Quel est l'impact de la longueur maximale d'entraînement pour les phrases plus longues que cette longueur (essayer par exemple avec 8) ?\n",
    "- Essayez d'ajouter des features morphologiques à ce modèle en concatenant les embeddings de mots avec les sorties d'un CNN sur les caractères du mot (couche de convolution + couche de pooling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
