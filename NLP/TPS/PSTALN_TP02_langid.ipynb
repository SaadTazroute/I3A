{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification de textes : identification de la langue\n",
    "============================\n",
    "\n",
    "La plupart de ces notebooks peuvent tourner sans acceleration GPU. Vous pouvez utiliser Google Colab ou installer jupyter-notebook sur votre ordinateur. Pytorch est déjà installé sur Colab, par contre il faut suivre https://pytorch.org/get-started/locally/ en local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example de deploiement local (pas besoin de refaire si vous avez déjà fait lors du TP1)\n",
    "# virtualenv -ppython3.8 pstaln-env\n",
    "# source pstaln-env/bin/activate\n",
    "# pip3 install torch==1.10.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "# pip3 install matplotlib ipykernel\n",
    "# python3 -m ipykernel install --user --name=pstaln-env\n",
    "# jupyter-notebook\n",
    "## puis sélectionner le noyeau pstaln-evn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14Pcq77vacyx"
   },
   "source": [
    "Notre objectif aujourd'hui est d'effectuer une classification de textes assez simple : étant donné un bout de texte, on voudrait prédire dans quelle langue ce texte a été écrit. Par exemple :\n",
    "\n",
    "$model(\\text{\"Vamos a la playa\"}) = \\text{ES}\\\\\n",
    "model(\\text{\"Je vois la vie en rose\"}) = \\text{FR}\\\\\n",
    "model(\\text{\"Ich hab den Farbfilm vergessen\"}) = \\text{DE}$\n",
    "\n",
    "Les codes ES, FR et DE sont les [codes ISO-639-2](https://www.loc.gov/standards/iso639-2/php/code_list.php) des langues espagnol, français et allemand. Nous travaillerons dans ce TP sur des langues européennes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Nous allons utiliser un dataset contenant des petits bouts de texte annotés avec le code de la langue. D'abord, téléchargeons le dataset qui a été préparé pour ce TP. Ce dataset provient d'un sujet de projet donné aux étudiant.e.s de L3 en informatique ([détails ici](https://wikitalep.lis-lab.fr/doku.php?id=projets-l3:langid))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "european-dev.txt\n",
      "european-test-ref.txt\n",
      "european-test.txt\n",
      "european-train.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d dataset-european/ ]; then\n",
    "  wget -q -O dataset-european.tar.gz https://pageperso.lis-lab.fr/carlos.ramisch/download_files/pstaln/dataset-european.tar.gz\n",
    "  tar xfz dataset-european.tar.gz \n",
    "  rm dataset-european.tar.gz\n",
    "fi\n",
    "ls dataset-european"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aussi, comme d'habitude, on importe les sous-modules de pytorch et d'autres modules python utiles dans ce TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OIW9J6pacy3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans chacun des fichiers texte du dataset (UTF-8, CR-LF), se trouve une liste de petits documents (une ou deux phrases la plupart du temps) suivis d'une tabulation, suivis du code de la langue à reconnaître (sauf dans le fichier `test`, où la langue n'est pas donnée, en mode `blind` d'une campagne d'évaluation). Nous pouvons ouvrir le fichier `train` pour faire quelques statistiques sur les données d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nb. languages: 21\n",
      "Total nb. documents: 85301\n"
     ]
    }
   ],
   "source": [
    "def opendataset(filename):\n",
    "    \"\"\"\n",
    "    returns a list of X input texts paired wiht a list of Y output language labels\n",
    "    \"\"\"\n",
    "    X = []; Y = []\n",
    "    with open(filename) as datasetfile:\n",
    "        for line in datasetfile :\n",
    "            (x,y) = line.strip().split(\"\\t\")\n",
    "            X.append(x); Y.append(y)\n",
    "    return (X,Y)\n",
    "\n",
    "(trainXtexts, trainYlabels) = opendataset(\"dataset-european/european-train.txt\")\n",
    "#showhistograms(trainX, trainY)\n",
    "hist=Counter(trainYlabels)\n",
    "plt.title(\"Documents per language in train\")\n",
    "plt.bar(hist.keys(),hist.values()); \n",
    "plt.show()\n",
    "print(\"Total nb. languages: {}\".format(len(hist)))\n",
    "print(\"Total nb. documents: {}\".format(len(trainXtexts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette base contient une grande quantité de documents par langue. Pour rendre les calculs plus rapides sur ce notebook, nous garderons un peu moins de 10% des textes. Si vous faites des expériences en local, vous pouvez conserver la totalité de la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nb. languages: 21\n",
      "Total nb. documents: 8000\n"
     ]
    }
   ],
   "source": [
    "random.seed(5)\n",
    "trainXtexts,trainYlabels = map(list,zip(*(random.sample(list(zip(trainXtexts,trainYlabels)),k=8000))))\n",
    "hist=Counter(trainYlabels)\n",
    "plt.title(\"Documents per language in train\")\n",
    "plt.bar(hist.keys(),hist.values()); \n",
    "plt.show()\n",
    "print(\"Total nb. languages: {}\".format(len(hist)))\n",
    "print(\"Total nb. documents: {}\".format(len(trainXtexts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des features\n",
    "\n",
    "La tâche d'identification de la langue est une tâche très simple, pour laquelle il est possible d'apprendre des modèles assez performants avec assez peu de features et de données d'entraînement. C'est pour cela que cette tâche est idéale pour commencer à effectuer des expériences avec pytorch sur le texte.\n",
    "\n",
    "La première chose à faire consiste à définir les features. Pour cela, nous allons nous concentrer sur les n-grammes de caractères, c'est-à-dire, les séquences de _n_ caractères dans un texte. Nous allons commencer avec $n=2$, donc des _bigammes_ de caractères.\n",
    "\n",
    "Voici un exemple d'une fonction qui estrait les n-grammes de caractères à partir d'une liste de textes. Le paramètre `n` détermine la longueur des n-grammes ; le paramètre `pad` permet d'activer un désactiver le padding, c'est-à-dire, l'insertion d'un caractère spécial pour marquer le début (`<`) et la fin (`>`) d'un texte ; le paramètre `lower` permet d'activer ou désactiver la transformation de tout le texte en minuscules avant toute extraction. On commence à voir une serie de pré-traitements nécessaires pour traiter le texte, tels que le padding et le traitement des majuscules/minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<a', 'ab', 'bc', 'cd', 'd_', '_a', 'ab', 'bc', 'cd', 'd>']\n",
      "Counter({'ab': 2, 'bc': 2, 'cd': 2, '<a': 1, 'd_': 1, '_a': 1, 'd>': 1})\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams(texts, n=2, pad=True, lower=True):\n",
    "    \"\"\"\n",
    "    Given a list of texts, returns a list of its n-grams in order of occurrence.    \n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    for text in texts :\n",
    "        if pad : # Padding -> add special beginning and end markers\n",
    "            text = \"<\" + text + \">\"\n",
    "        if lower : # Lowercase all text before extracting n-grams\n",
    "            text = text.lower()\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngrams.append(text.replace(\" \",\"_\")[i:i + n]) # Space replaced by underscore\n",
    "    return ngrams\n",
    "\n",
    "# testez en variant les valeurs de n, pad, lower\n",
    "ngrams_toy = get_ngrams([\"abcd ABCD\"],n=2,pad=True,lower=True)\n",
    "print(ngrams_toy) \n",
    "print(Counter(ngrams_toy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut maintenant une liste des bigrammes de caractères les plus fréquents par langue. Pour cela, nous allons regrouper les textes dans un dictionnaire dont les clés sont les codes de langue et les valeurs sont des listes de textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je donne la parole à M. le président en exercice du Conseil.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsperlang = defaultdict(list)\n",
    "for (x,y) in zip(trainXtexts,trainYlabels) :\n",
    "    textsperlang[y].append(x)\n",
    "textsperlang[\"fr\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire `textsperlang` contient désormais tous les textes d'entraînement regroupés par langue. Nous allons maintenant créer un ensemble de features (ou vocabulaire), constitué de l'union des N bigrammes les plus fréquents par langue. Pour aller plus vite, nous allons utiliser uniquement les 300 premiers textes de chaque langue pour extraire ces features, cela devrait suffire pour avoir un échantillon représentatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vocabulary: ['ke', 'ri', 'ι_', '_г', 'nu', 'kö', 'jā', 'от', 'ου', 'ис']...\n",
      "Number of features: 551\n"
     ]
    }
   ],
   "source": [
    "N=100 # retain N most frequent n-grams per language\n",
    "n=2   # size of n-gram is 2\n",
    "# You can play with these parameters later to see how they influence the predictions\n",
    "    \n",
    "vocab = set([])\n",
    "for lang in textsperlang.keys() :\n",
    "    freq_ngrams = Counter(get_ngrams(textsperlang[lang][:300],n=n)).most_common(N)    \n",
    "    vocab = vocab | set(map(lambda x:x[0],freq_ngrams))\n",
    "    #print(lang, freq_ngrams[:5]) # Uncomment if you want to see most common n-grams per lang\n",
    "\n",
    "vocab = list(vocab)\n",
    "print(\"Feature vocabulary: {}...\".format(vocab[:10]))\n",
    "print(\"Number of features: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation one-hot\n",
    "\n",
    "Nous allons maintenant passer à l'étape de création du classifieur MLP avec pytorch. Pour cela, il faudra représenter chaque texte sous la forme d'un vecteur qui est à la fois utile pour notre tâche et traitable dans un modèle de ce type. \n",
    "\n",
    "La première approche consiste à représenter chaque ngramme de caractères sous la forme d'un vecteur _one-hot_, c'est-à-dire, un vecteur de 552 positions constitué uniquement de zéros, sauf à une position _i_ correspondant au n-gramme de caractères représenté. \n",
    "\n",
    "$$x \\in \\mathbb{R}^{552}, x{[i]} = \\begin{cases}1&\\text{si c'est le i-ème n-gramme}\\\\0&\\text{sinon}\\end{cases}$$\n",
    "\n",
    "Cela veut dire que chaque n-gramme sera associé à un indice $i$ du vecteur $x$. Cependant, notez que le nombre de features peut vite exploser. Par exemple, un texte contenant 1000 n-grammes sera représenté par une suite de $552\\times 1000 = 552000$ features.\n",
    "\n",
    "De plus, comme chaque texte contient un nombre variable de caractères et donc de n-grammes, il faudra trouver une astuce pour que toutes les entrées du réseau aient la même longueur. Il est possible de faire cela avec du padding (que nous verrons plus tard) mais pour le moment, nous allons adopter une stratégie plus simple, qui consiste à faire un \"sac de n-grammes\" (la version la plus connue de ce type d'approche s'appelle \"sac de mots\" ou \"bag of words\" en anglais).\n",
    "\n",
    "Ainsi, notre représentation du document $D$ ignorera l'ordre dans lequel les n-grammes apparaissent, mais représentera uniquement l'apparition du n-gramme dans le document :\n",
    "\n",
    "$$d \\in \\mathbb{R}^{552}, d{[i]} = \\begin{cases}1&\\text{si $D$ contient le n-gramme}~i\\\\0&\\text{sinon}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def get_onehot(features,vocab):\n",
    "    \"\"\"\n",
    "    Create a one-hot torch tensor of dimension `len(vocab)` with ones at positions present in `features`\n",
    "    `vocab` is the list of all possible features, position in list encodes feature index\n",
    "    \"\"\"\n",
    "    onehot =  torch.zeros(len(vocab))\n",
    "    for (i,feat) in enumerate(vocab) :\n",
    "        if feat in features :\n",
    "            onehot[i] = 1.0\n",
    "    return onehot\n",
    "\n",
    "print(get_onehot(set(get_ngrams([\"le monde est grand\"])),vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'accélérer les calculs plus tard, nous allons sauvegarder, pour chaque document, son histogramme de n-grammes de caractères. Ensuite, nous passons la liste de n-grammes présents dans le document pour l'encodage one-hot. Cette étape peut prendre un peu de temps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<t', 'tr', 're', 'et', 'tj'] -> sl\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([8000, 551])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "trainXngrams = [get_ngrams([x]) for x in trainXtexts]\n",
    "print(list(trainXngrams[0])[:5],\"->\",trainYlabels[0])\n",
    "\n",
    "trainX = torch.stack([get_onehot(set(x),vocab) for x in trainXngrams])\n",
    "print(trainX[0,:10])\n",
    "print(trainX.shape)\n",
    "print(trainX.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre classifieur multi-classes, il faudra également encoder chaque langue sous la forme d'un indice entier. Nous allons donc créer un dictionnaire qui renvoie un entier pour chaque code de langue, et ensuite appliquer la transformation pour obtenir `Y` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 1, 6, 7, 8])\n",
      "torch.Size([8000])\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "langs = list(textsperlang.keys())\n",
    "lang2id = {lang:i for (i,lang) in enumerate(langs)}\n",
    "id2lang = {i:lang for (i,lang) in enumerate(langs)}\n",
    "\n",
    "trainY = torch.tensor([lang2id[y] for y in trainYlabels])\n",
    "print(trainY[:10])\n",
    "print(trainY.shape)\n",
    "print(trainY.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieur\n",
    "\n",
    "Nos données sont désormais prêtes. Nous allons apprendre un classifieur très simple, composé d'une transformation linéaire de nos vecteurs $x$ dimension 552, vers un vecteur de dimension 21 représentant les langues, puis d'un softmax sur ces derniers (inclus dans la loss). Voyons déjà ce que ce modèle sera capable de faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_set = TensorDataset(trainX, trainY)\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,dimX,dimY):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(dimX, dimY)\n",
    "    def forward(self, x):\n",
    "        return self.layer1(x)\n",
    "    \n",
    "langIDv1 = SimpleModel(len(vocab),len(langs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la fonction d'apprentissage, nous utiliserons la même que celle du TP précédant. Voyons ce que ça donne..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.057755003814352675\n",
      "2 0.0034132687800665735\n",
      "4 0.0012113664920607335\n",
      "6 0.0005010913811964883\n",
      "8 0.0002298640650243442\n",
      "10 0.00011539934953624708\n",
      "12 6.350934430631128e-05\n",
      "14 3.78047396104062e-05\n",
      "16 2.352396462466022e-05\n",
      "18 1.4902860520004336e-05\n"
     ]
    }
   ],
   "source": [
    "def fit(model, epochs, train_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0; num = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(x)  \n",
    "            loss = criterion(y_scores, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num += len(y)\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            print(epoch, total_loss / num)\n",
    "            \n",
    "fit(langIDv1,20,train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que vaut ce classifieur ? Pour l'évaluer, on peut prédire la langue des textes de l'ensemble de développement (ou validation), et calculer le taux de succès (_accuracy_ ou exactitude). Pour cela, nous allons créer une fonction `predict` qui prédit la langue d'une liste de textes. Notez la ligne `model.eval()` -- elle permet de mettre le modèle en mode évaluation, par exemple, pour ne pas appliquer de dropout si il y en a. Le modèle est remis en mode train à la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'fr', 'de']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def predict(model, texts, vocab, id2lang):\n",
    "    model.eval()\n",
    "    ngrams = [get_ngrams([x]) for x in texts]    \n",
    "    X = torch.stack([get_onehot(set(x), vocab) for x in ngrams])\n",
    "    Yhat = torch.max(model(X),1)[1]\n",
    "    model.train()\n",
    "    return [id2lang[y.item()] for y in Yhat]\n",
    "\n",
    "def accuracy(predict,ref):\n",
    "    return sum([1 if y == yhat else 0 for (yhat,y) in zip(predict,ref)])/len(ref)\n",
    "\n",
    "pred = predict(langIDv1,[\"Vamos a la playa\",\"je vois la vie en rose\",\"Ich hab den Farbfilm vergessen\"],vocab,id2lang)\n",
    "print(pred)\n",
    "print(accuracy(pred,[\"es\",\"fr\",\"de\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un premier résultat prometteur. Voyons le résultat sur la totalité du dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9810\n"
     ]
    }
   ],
   "source": [
    "(devXtexts, devYlabels) = opendataset(\"dataset-european/european-dev.txt\")\n",
    "\n",
    "pred=predict(langIDv1,devXtexts,vocab,id2lang)\n",
    "print(\"accuracy {:.4f}\".format(accuracy(pred,devYlabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, impressionnant ! Un modèle très simple est capable d'identifier la langue des textes avec une exactitude d'environ 98%. On pourrait rendre la tâche artificiellement plus dûre. Pour cela, nous pouvons garder uniquement les 15 premiers caractères de notre texte, car il est beaucoup plus difficile de prédire la langue d'un texte court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8254\n"
     ]
    }
   ],
   "source": [
    "devXtextshard = [x[:15] for x in devXtexts]\n",
    "pred = predict(langIDv1, devXtextshard, vocab, id2lang)\n",
    "print(\"accuracy {:.4f}\".format(accuracy(pred,devYlabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifieur MLP avec embeddings\n",
    "\n",
    "Une alternative plus souple aux représentations one-hot est la représentation de chaque n-gramme de caractères comme un vecteur dense de plus petite dimension (par exemple, 50 au lieu de 552). Ce vecteur, noté $e(D[i])$ pour le n-gramme correspondant à la position $i$ du document $D$, est initialisé aléatoirement et ses poids sont ajustés lors de l'entraînement. Pour représenter un document, nous allons procéder de manière similaire, en calcular un vecteur moyen des bigrammes contenus dans le document $D$ :\n",
    "\n",
    "$$d \\in \\mathbb{R}^{50}, d = \\frac{1}{|D|}\\sum_{i=1}^{|D|} e(D[i])$$\n",
    "\n",
    "Notez que dans cette représentation, une information supplémentaire est implicitement représentée : la fréquence d'occurrence de chaque n-gramme dans le document. Plus un n-gramme est fréquent dans le document, plus son poids augment dans la moyenne.\n",
    "\n",
    "Pour implémenter cette stratégie, nous pouvons utiliser la couche `nn.Embedding` de pytorch. Elle prend en entrée deux paramètres : la taille du vocabulaire et la dimension des embeddings. Lors de la prédiction, elle renvoie les embeddings correspondant à une liste d'indices donnés en entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 50\n",
    "\n",
    "class LinearModelEmbed(nn.Module):\n",
    "    def __init__(self,dimX,dimY):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(dimX,embed_dim)        \n",
    "        self.layer1 = nn.Linear(embed_dim, dimY)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed(x)              # convert each n-gram index into 50-d vector\n",
    "        combined = torch.mean(embedded,dim=0) # average embeddings for the whole document\n",
    "        return self.layer1(combined)\n",
    "    \n",
    "langIDv2 = LinearModelEmbed(len(vocab)+1,len(langs)) # len(vocab)+1 pour prendre en compte le padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il sera aussi nécessaire de transformer nos entrées : au lieu d'avoir un tenseur avec des représentations one-hot, chaque document doit maintenant être représenté par une liste d'indices correspondant aux bigrammes qu'il contient. Pour cela, nous allons utiliser un `LongTensor`. En raison de l'entraînement par batches, il faudra couper les textes pour qu'ils ne dépassent pas une longueur maximale. De même, nous utiliserons le symbole spécial `0` pour représenter le padding, c'est-à-dire, pour remplir la séquence si elle contient moins de `seq_max_len` mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([353, 395,  28,  43, 273, 389,  53,  24, 232,  88,  78, 417,  86, 108,\n",
      "        292,  48, 273, 201, 143, 333, 480, 489,  30, 450,   2, 176, 108, 523,\n",
      "        341, 544,  24, 542, 168, 317, 368,  14, 368, 278, 183, 351, 165,  99,\n",
      "         59, 198, 317, 109, 207, 536, 337, 202, 460, 247,  24, 143, 154, 247,\n",
      "         20, 309, 432,  30, 450, 210,  53,  20, 165,  99, 538,  30, 450, 395,\n",
      "        232, 376, 232,  70,  84, 202, 368,  24,  28, 535,  78, 203, 411,  48,\n",
      "        381, 508,  48, 304, 524, 504, 378, 247,  20, 423, 460, 246,  43, 264,\n",
      "        428,  78,  82, 278,  48, 523,  24,  94, 376, 419, 388,  43, 315,   1,\n",
      "        183,  25, 368, 403, 202, 523, 341, 470, 271,  28, 178, 536, 368, 305,\n",
      "        450, 395])\n",
      "torch.Size([8000, 128])\n"
     ]
    }
   ],
   "source": [
    "seq_max_len = 128\n",
    "bigram2id = {bigram:i+1 for (i,bigram) in enumerate(vocab)}    \n",
    "\n",
    "def get_indices(features, bigram2id):\n",
    "    \"\"\"\n",
    "    Create a Long tensor of dimension `seq_max_len` with indices of features in vocab\n",
    "    \"\"\"\n",
    "    indices = torch.zeros(seq_max_len) \n",
    "    indices_pos = 0\n",
    "    for k in features :\n",
    "        if k in bigram2id.keys() :\n",
    "            indices[indices_pos] = bigram2id[k]\n",
    "            indices_pos += 1\n",
    "            if indices_pos == seq_max_len : \n",
    "                break # cut sentence here\n",
    "    if len(indices) < seq_max_len :\n",
    "        indices = indices + [0 * (seq_max_len-len(indices))]\n",
    "    return indices.long()\n",
    "\n",
    "trainX = torch.stack([get_indices(x,bigram2id) for x in trainXngrams])\n",
    "print(trainX[0])\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînons maintenant ce modèle, mais cette fois-ci nous nous intéressons à la loss et à l'exactitude sur le jeu de validation tout au long de l'entraînement, pour avoir une idée de la courbe d'apprentissage sur le jeu de développement. Ainsi, nous créons une nouvelle fonction d'entraînement qui, toutes les 1/10 époques, calcule et affiche la loss et l'exactitude sur le jeu de développement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train-loss 9.753850403538932e-06, val-loss 0.007925681402360761 val-accuracy 0.9841\n",
      "epoch 2, train-loss 6.468906471771008e-06, val-loss 0.008608369242698347 val-accuracy 0.9810\n",
      "epoch 4, train-loss 4.3512719995298936e-06, val-loss 0.009024225197127015 val-accuracy 0.9778\n",
      "epoch 6, train-loss 2.875709556538486e-06, val-loss 0.009267665824098947 val-accuracy 0.9778\n",
      "epoch 8, train-loss 1.8662878828860086e-06, val-loss 0.009667157958991725 val-accuracy 0.9778\n",
      "epoch 10, train-loss 1.2612531721206688e-06, val-loss 0.01000777374142722 val-accuracy 0.9778\n",
      "epoch 12, train-loss 8.055187451819767e-07, val-loss 0.009761317374908741 val-accuracy 0.9841\n",
      "epoch 14, train-loss 5.137598807244404e-07, val-loss 0.010559997679343276 val-accuracy 0.9778\n",
      "epoch 16, train-loss 3.4854141125506556e-07, val-loss 0.010506865877157337 val-accuracy 0.9810\n",
      "epoch 18, train-loss 2.277376015503707e-07, val-loss 0.010517740220465215 val-accuracy 0.9810\n"
     ]
    }
   ],
   "source": [
    "def accuracy_and_loss(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = correct = num = 0\n",
    "    for x, y in loader:\n",
    "        with torch.no_grad(): # no gradient needed\n",
    "            y_scores = model(x)\n",
    "            loss = criterion(y_scores, y)\n",
    "            y_pred = torch.max(y_scores, 1)[1]\n",
    "            correct += torch.sum(y_pred.data == y)\n",
    "            total_loss += loss.item()\n",
    "            num += len(y)\n",
    "    model.train()\n",
    "    return total_loss / num, correct.item() / num\n",
    "\n",
    "def fit_curve(model, epochs, train_loader, dev_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0; num = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(x)  \n",
    "            loss = criterion(y_scores, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num += len(y)\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            acc = accuracy_and_loss(model, dev_loader)\n",
    "            template = \"epoch {}, train-loss {}, val-loss {} val-accuracy {:.4f}\"\n",
    "            print(template.format(epoch, total_loss / num, *acc))\n",
    "\n",
    "devY = torch.tensor([lang2id[y] for y in devYlabels])\n",
    "devXngrams = [get_ngrams([x]) for x in devXtexts]\n",
    "devX = torch.stack([get_onehot(x,bigram2id) for x in devXngrams])\n",
    "dev_set = TensorDataset(devX, devY)\n",
    "dev_loader = DataLoader(dev_set, batch_size=8)\n",
    "\n",
    "fit_curve(langIDv1,20,train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que le classifieur converge très vite et obtient une performance similaire à celle du classifieur avec des représentations one-hot. En fait, on peut considérer la matrice $W$ du classifieur avec représentations one-hot comme une couche d'embeddings de dimension 21, car chaque vecteur one-hot multiplié par cette matrice donne un vecteur de dimension 21 le représentant de manière dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqYUPLtcac0z"
   },
   "source": [
    "Exercice 1\n",
    "---------\n",
    "\n",
    "Une représentation alternative consiste à utiliser des vecteurs TF ou TF.IDF à la place des embeddings initialisés aléatoirement. Les vecteurs TF ressemblent aux vecteurs one-hot mais, à la place des 1, ils contiennent la fréquence du n-gramme, c'est-à-dire son nombre d'occurrences dans le document divisé par la longueur du document. Le terme IDF peut être multiplié à TF. Le terme IDF correspond à l'inverse du nombre de documents contenant le bigramme, souvent multiplié par un log. Formellement :\n",
    "  - TF (term frequency): $x[i] = \\frac{nb_D(i)}{|D|}$ où $nb_D(i)$ est le nombre d'occurrences du n-gramme $i$ dans le document $D$ et $|D|$ la longueur du document\n",
    "  - TF.IDF (term frequency - inverse document frequency): $x[i] = \\frac{nb_D(i)}{|D|}\\times \\log \\frac{1}{doc_{}(i)}$ où TF est comme ci-dessus, et $doc_{}(i)$ est le nombre de documents dans la base contenant au moins une occurrence du n-gramme $i$\n",
    "\n",
    "Construisez un classifieur qui prend en entrée des représentations TF ou TF.IDF - est-ce que ça marche aussi bien, mieux, moins bien ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "-------\n",
    "\n",
    "Est-il possible de faire un modèle presque aussi performant en diminuant :\n",
    "  - le nombre de features/paramètres ?\n",
    "  - la taille des données d'entraînement ?\n",
    "  - le nombre d'époques ?\n",
    "  \n",
    "Essayez de créer le modèle le plus compact possible tout en ne perdant pas trop en exactitude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDXEl6Qzac00",
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin\n",
    "------------\n",
    "\n",
    "* Une façon efficace d'extraire des features sans devoir limiter le jeu de n-grammes est d'utiliser la convolution sur les caractères. Faites un classifieur qui prent en entrée une liste de caractères représentés sous la forme d'embeddings de petite dimension et qui applique des convolutions sur ces caractères pour extraire des bigrammes pertinents.\n",
    "\n",
    "* Des modèles d'apprentissage très simples tels que les classifieurs du type KNN ou K-means marchent bien pour cette tâche. En utilisant une représentation du type TF ou TF-IDF, essayez d'effectuer la même tâche avec un classifieur non neuronal (p.ex. dans scikit-learn). Est-ce que vous arrivez à obtenir des performances proches de celle des classifieurs linéaires de ce TP ?\n",
    "\n",
    "* La tâche d'identification de la langue est facile avec un nombre limité de langues assez différentes pour lesquelles il y a pas mal de matériel d'entraînement. Essayez de résoudre ce problème avec les variantes proposées sur ce site, pour les langues proches ou les dialectes : [https://wikitalep.lis-lab.fr/doku.php?id=projets-l3:langid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_pytorch-mlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pstaln-env",
   "language": "python",
   "name": "pstaln-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
