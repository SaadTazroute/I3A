{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cVJjtG3M5O9O",
    "outputId": "e58973c2-8b7c-457c-bf52-85694b0ac2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/gabriel/anaconda3/lib/python3.7/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/gabriel/anaconda3/lib/python3.7/site-packages (from torch) (1.16.4)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAiCT1Oi6iG_"
   },
   "source": [
    "## Téléchargement et format des données\n",
    "\n",
    "Nous avons un fichier a 7 colonnes\n",
    "\n",
    "MOTS ---- POS ---- HEAD_ID ---- DEPREL ---- TARGET_OR_NOT ---- LABEL_ENCODE_1 ---- LABEL_ENCODE_2\n",
    "\n",
    "---\n",
    "Les 4 premières vous les connaissez déjà, elles ont été predites automatiquement avec un analyzeur syntaxique (UDPipe)\n",
    "\n",
    "Les 3 dernières sont des colonnes associées à la semantique en frames (FrameNet)\n",
    "\n",
    "**TARGET_OR_NOT**: indique si le mot peut déclancher une frame (T) ou pas (N)\n",
    "\n",
    "**LABEL_ENCODE_1**: une façon de encoder les labels semantiques     [BIO] + [FE ou TG] + [ROLE or FRAME]\n",
    "\n",
    "**LABEL_ENCODE_2**: une deuxième façon de encoder les labels semantiques     [BIO] + FRAME_NAME + [FE ou TG] + [ROLE or LEXICAL_UNIT]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Dans ce fichier il y a déjà eu un pretraitement pour selectioner quels sont les TARGET possibles. Si une phrase a 2 TARGETS ou plus elle a été dupliqué autant de fois qu'il y avait de TARGETS, pour pouvoir traiter un TARGET à la fois.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "9W8DgSn35VmW",
    "outputId": "2a93592a-181f-4fe1-b377-1a9ce233e075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Même\tADV\t2\tadvmod\tN\tO\tO\n",
      "si\tSCONJ\t7\tmark\tN\tO\tO\n",
      "leur\tDET\t4\tnmod:poss\tN\tB:FE:Product\tB:Subjective_influence:FE:Product\n",
      "culture\tNOUN\t7\tnsubj:pass\tN\tI:FE:Product\tI:Subjective_influence:FE:Product\n",
      "a\tAUX\t7\taux\tN\tO\tO\n",
      "été\tAUX\t7\taux:pass\tN\tO\tO\n",
      "influencée\tVERB\t30\tadvcl\tT\tB:TG:Subjective_influence\tB:Subjective_influence:TG:influencer\n",
      "par\tADP\t10\tcase\tN\tO\tO\n",
      "leurs\tDET\t10\tnmod:poss\tN\tB:FE:Entity\tB:Subjective_influence:FE:Entity\n",
      "voisins\tNOUN\t7\tobl\tN\tI:FE:Entity\tI:Subjective_influence:FE:Entity\n",
      "dans\tADP\t14\tcase\tN\tB:FE:Domain\tB:Subjective_influence:FE:Domain\n",
      "de\tDET\t14\tdet\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "nombreux\tADJ\t14\tamod\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "domaines\tNOUN\t7\tobl\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "(\tPUNCT\t16\tpunct\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "musique\tNOUN\t14\tappos\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      ",\tPUNCT\t18\tpunct\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "cuisine\tNOUN\t16\tconj\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      ",\tPUNCT\t20\tpunct\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n",
      "littérature\tNOUN\t16\tconj\tN\tI:FE:Domain\tI:Subjective_influence:FE:Domain\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -f frames.data ]; then\n",
    "    wget -q https://pageperso.lis-lab.fr/gabriel.marzinotto/frames.data\n",
    "fi\n",
    "head -20 frames.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPe_M7zQ9JPU"
   },
   "source": [
    "La première chose à faire est charger les données. Un premier tableau contiendra les mots de chaque phrase, et un second les POS, puis un pour TARGET_OR_NOT et pous de tableaux pour les étiquettes encodées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "JQ1mDg0F9EcB",
    "outputId": "3e92e53a-2e3e-4c18-a87a-ff8b0d7b95eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7605\n",
      "['Les', 'débuts', 'de', 'la', 'néolithisation', 'de', '9500', 'à', '6000', 'av.']\n",
      "['O', 'B:TG:Activity_start', 'B:FE:Activity', 'I:FE:Activity', 'I:FE:Activity', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "list_text = []\n",
    "list_pos = []\n",
    "list_tg_or_not = []\n",
    "list_labels_1 = []\n",
    "list_labels_2 = []\n",
    "\n",
    "with open('frames.data') as fp:\n",
    "    text = []\n",
    "    pos = []\n",
    "    tg_tag = []\n",
    "    label_1 = []\n",
    "    label_2 = []\n",
    "    \n",
    "    for line in fp:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        #print(tokens)\n",
    "        if len(tokens) > 1:\n",
    "            text.append(tokens[0])\n",
    "            pos.append(tokens[1])\n",
    "            tg_tag.append(tokens[4])\n",
    "            label_1.append(tokens[5])\n",
    "            label_2.append(tokens[6])\n",
    "\n",
    "        else:\n",
    "            list_text.append(text)\n",
    "            list_pos.append(pos)\n",
    "            list_tg_or_not.append(tg_tag)\n",
    "            list_labels_1.append(label_1)\n",
    "            list_labels_2.append(label_2)\n",
    "\n",
    "            text = []\n",
    "            pos = []\n",
    "            tg_tag = []\n",
    "            label_1 = []\n",
    "            label_2 = []\n",
    "\n",
    "print(len(list_text))\n",
    "\n",
    "print(list_text[12])\n",
    "print(list_labels_1[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1FnJ8Xi_0Ip"
   },
   "source": [
    "Il faut ensuite convertir les étiquettes et les mots en entiers. Nous allons devoir créer des séquences de taille fixe, donc il faut réserver une étiquette de padding, ici `<eos>`. Toutefois, pour l'instant nous nous contentons de convertir les étiquettes sans appliquer le padding. Un defaultdict est utilisé pour créer le vocabulaire des étiquettes. À chaque fois qu'une nouvelle étiquette est rencontrée, il l'associe à une nouvelle valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "f5yv9IyU_KFA",
    "outputId": "5499aaae-3efd-4157-9c52-7563a8d50550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 12, 13, 13, 1, 1, 1, 1, 1]\n",
      "defaultdict(<function <lambda> at 0x7f4289307d08>, {'<eos>': 0, 'O': 1, 'B:FE:Product': 2, 'I:FE:Product': 3, 'B:TG:Subjective_influence': 4, 'B:FE:Entity': 5, 'I:FE:Entity': 6, 'B:FE:Domain': 7, 'I:FE:Domain': 8, 'B:FE:Cognizer': 9, 'I:FE:Cognizer': 10, 'B:TG:Activity_start': 11, 'B:FE:Activity': 12, 'I:FE:Activity': 13, 'B:FE:Co-timed_event': 14, 'I:FE:Co-timed_event': 15, 'B:FE:Time': 16, 'I:FE:Time': 17, 'B:TG:Killing': 18, 'B:FE:Victim': 19, 'B:TG:Origin': 20, 'B:FE:Instrument': 21, 'I:FE:Instrument': 22, 'B:TG:Using': 23, 'B:FE:Purpose': 24, 'I:FE:Purpose': 25, 'B:FE:Origin': 26, 'I:FE:Origin': 27, 'B:FE:Colonists': 28, 'I:FE:Colonists': 29, 'B:TG:Colonization': 30, 'B:FE:New_area': 31, 'I:FE:New_area': 32, 'B:FE:Sought_entity': 33, 'I:FE:Sought_entity': 34, 'B:TG:Locating': 35, 'B:TG:Scrutiny': 36, 'B:FE:Phenomenon': 37, 'I:FE:Phenomenon': 38, 'B:FE:Circumstances': 39, 'I:FE:Circumstances': 40, 'B:TG:Existence': 41, 'B:FE:Explanation': 42, 'I:FE:Explanation': 43, 'B:TG:Becoming_aware': 44, 'B:FE:Ground': 45, 'I:FE:Ground': 46, 'B:FE:Theme': 47, 'I:FE:Theme': 48, 'B:FE:Source': 49, 'I:FE:Source': 50, 'B:TG:Arriving': 51, 'B:FE:Goal': 52, 'I:FE:Goal': 53, 'B:TG:Finish_competition': 54, 'B:FE:Opponent': 55, 'I:FE:Opponent': 56, 'B:FE:Place': 57, 'I:FE:Place': 58, 'B:TG:Buildings': 59, 'B:FE:Created_entity': 60, 'I:FE:Created_entity': 61, 'B:TG:Becoming': 62, 'B:FE:Final_quality': 63, 'B:TG:Attack': 64, 'B:FE:Assaillant': 65, 'I:FE:Assaillant': 66, 'I:FE:Victim': 67, 'B:TG:Motion': 68, 'B:FE:Path': 69, 'I:FE:Path': 70, 'B:FE:Competitor': 71, 'I:FE:Competitor': 72, 'B:FE:Donor': 73, 'I:FE:Donor': 74, 'B:TG:Giving': 75, 'B:TG:Awareness': 76, 'B:FE:Content': 77, 'I:FE:Content': 78, 'B:FE:Agent': 79, 'I:FE:Agent': 80, 'B:FE:Helper': 81, 'B:TG:Assistance': 82, 'B:FE:Participant1': 83, 'B:TG:Participation': 84, 'B:FE:Event': 85, 'I:FE:Event': 86, 'B:FE:Competitors': 87, 'I:FE:Competitors': 88, 'B:FE:Final_category': 89, 'I:FE:Final_category': 90, 'B:FE:Addressee': 91, 'I:FE:Addressee': 92, 'B:FE:Speaker': 93, 'I:FE:Speaker': 94, 'B:TG:Statement': 95, 'B:FE:Message': 96, 'I:FE:Message': 97, 'B:FE:Protagonist': 98, 'B:TG:Death': 99, 'B:TG:Sending': 100, 'B:FE:Perceiver': 101, 'B:FE:Location': 102, 'I:FE:Location': 103, 'B:FE:Topic': 104, 'I:FE:Topic': 105, 'B:TG:Installing': 106, 'B:FE:Component': 107, 'I:FE:Component': 108, 'B:FE:Fixed_location': 109, 'I:FE:Fixed_location': 110, 'B:TG:Age': 111, 'B:FE:Age': 112, 'B:FE:Competition': 113, 'I:FE:Competition': 114, 'B:FE:Leader': 115, 'I:FE:Leader': 116, 'B:TG:Leadership': 117, 'B:FE:Governed': 118, 'I:FE:Governed': 119, 'B:FE:Creator': 120, 'I:FE:Creator': 121, 'B:TG:Creating': 122, 'B:TG:Hostile_encounter': 123, 'B:FE:Result': 124, 'I:FE:Result': 125, 'I:FE:Protagonist': 126, 'B:FE:Carrier': 127, 'B:FE:Benefited_party': 128, 'I:FE:Benefited_party': 129, 'I:FE:Helper': 130, 'B:FE:Cognizer_agent': 131, 'I:FE:Cognizer_agent': 132, 'B:TG:Seeking': 133, 'B:TG:Departing': 134, 'I:FE:Perceiver': 135, 'B:TG:Deciding': 136, 'B:FE:Decision': 137, 'I:FE:Decision': 138, 'B:FE:Sender': 139, 'I:FE:Sender': 140, 'B:FE:Possession': 141, 'I:FE:Possession': 142, 'B:TG:Losing': 143, 'B:FE:Owner': 144, 'I:FE:Owner': 145, 'B:TG:Change_of_leadership': 146, 'B:FE:Function': 147, 'I:FE:Function': 148, 'I:FE:Age': 149, 'B:FE:Influencing_situation': 150, 'I:FE:Influencing_situation': 151, 'B:TG:Objective_influence': 152, 'B:FE:Dependent_situation': 153, 'I:FE:Dependent_situation': 154, 'B:FE:Selector': 155, 'I:FE:Selector': 156, 'B:FE:New_leader': 157, 'B:TG:Making_arrangements': 158, 'B:FE:Means': 159, 'B:FE:Killer': 160, 'I:FE:Killer': 161, 'B:FE:Recipient': 162, 'I:FE:Participant1': 163, 'I:FE:Recipient': 164, 'B:FE:Body': 165, 'I:FE:Body': 166, 'B:TG:Request': 167, 'B:FE:Manner': 168, 'B:FE:Beneficiary': 169, 'I:FE:Beneficiary': 170, 'B:FE:Suspect': 171, 'I:FE:Suspect': 172, 'B:TG:Arrest': 173, 'I:FE:New_leader': 174, 'B:TG:Choosing': 175, 'B:FE:Chosen': 176, 'I:FE:Chosen': 177, 'B:FE:Communicator': 178, 'I:FE:Communicator': 179, 'B:TG:Expressing_publicly': 180, 'B:FE:Object': 181, 'I:FE:Object': 182, 'B:TG:Dimension': 183, 'B:FE:Locus': 184, 'I:FE:Locus': 185, 'B:FE:Total': 186, 'I:FE:Total': 187, 'B:TG:Inclusion': 188, 'B:FE:Part': 189, 'I:FE:Part': 190, 'B:FE:Side1': 191, 'B:FE:Side2': 192, 'I:FE:Side2': 193, 'B:FE:Cause': 194, 'I:FE:Cause': 195, 'B:FE:Measurement': 196, 'I:FE:Measurement': 197, 'B:FE:Subject': 198, 'B:TG:Education_teaching': 199, 'B:FE:Agent#Components': 200, 'I:FE:Agent#Components': 201, 'B:FE:Components': 202, 'I:FE:Components': 203, 'B:FE:Unconfirmed_content': 204, 'B:TG:Verification': 205, 'I:FE:Unconfirmed_content': 206, 'B:FE:Situation': 207, 'B:FE:Hidden_object': 208, 'I:FE:Hidden_object': 209, 'B:TG:Hiding_objects': 210, 'B:FE:Role': 211, 'I:FE:Role': 212, 'B:FE:Inherent_purpose': 213, 'I:FE:Inherent_purpose': 214, 'B:FE:Student': 215, 'I:FE:Student': 216, 'B:FE:Institution': 217, 'I:FE:Institution': 218, 'B:TG:Coming_to_believe': 219, 'I:FE:Means': 220, 'I:FE:Subject': 221, 'B:FE:Material': 222, 'I:FE:Material': 223, 'B:FE:Attribute': 224, 'I:FE:Attribute': 225, 'B:FE:ARGM-LOC': 226, 'I:FE:ARGM-LOC': 227, 'B:FE:ARGM-TMP': 228, 'I:FE:ARGM-TMP': 229, 'B:TG:Accomplishment': 230, 'B:FE:Inspector': 231, 'I:FE:Inspector': 232, 'B:FE:Duration': 233, 'I:FE:Duration': 234, 'B:TG:Hunting': 235, 'B:FE:Hunter': 236, 'B:FE:Dimension': 237, 'I:FE:Dimension': 238, 'B:FE:Ingestibles': 239, 'B:TG:Ingestion': 240, 'B:FE:Ingestor': 241, 'I:FE:Ingestor': 242, 'B:FE:Expressor': 243, 'I:FE:Expressor': 244, 'B:FE:Food': 245, 'I:FE:Food': 246, 'B:FE:Distance': 247, 'I:FE:Distance': 248, 'I:FE:Manner': 249, 'B:FE:Duration_of_final_state': 250, 'I:FE:Duration_of_final_state': 251, 'B:FE:Official': 252, 'B:TG:Appointing': 253, 'B:TG:Contacting': 254, 'B:FE:Communication': 255, 'I:FE:Communication': 256, 'B:TG:Coming_up_with': 257, 'B:FE:Idea': 258, 'I:FE:Idea': 259, 'I:FE:Ingestibles': 260, 'B:FE:Medium': 261, 'I:FE:Side1': 262, 'B:FE:Course': 263, 'B:FE:Participants': 264, 'I:FE:Participants': 265, 'B:FE:Co-participant': 266, 'I:FE:Co-participant': 267, 'B:TG:Shoot_projectiles': 268, 'B:FE:Projectile': 269, 'I:FE:Projectile': 270, 'I:FE:Course': 271, 'B:FE:Fact': 272, 'I:FE:Fact': 273, 'B:FE:Homeland': 274, 'B:FE:ARG0': 275, 'I:FE:ARG0': 276, 'I:FE:Official': 277, 'B:FE:Cotheme': 278, 'I:FE:Cotheme': 279, 'I:FE:Homeland': 280, 'B:FE:ARG1': 281, 'I:FE:ARG1': 282, 'I:FE:Medium': 283, 'B:FE:Teacher': 284, 'I:FE:Teacher': 285, 'B:FE:Area': 286, 'I:FE:Area': 287, 'B:FE:Containing_event': 288, 'I:FE:Containing_event': 289, 'I:FE:Hunter': 290, 'B:FE:Degree': 291, 'I:FE:Degree': 292, 'I:FE:Final_quality': 293, 'I:FE:Situation': 294, 'B:TG:Conduct': 295, 'B:FE:Participant2': 296, 'I:FE:Participant2': 297, 'B:FE:Behavior': 298, 'I:FE:Behavior': 299, 'B:FE:Affected_party': 300, 'I:FE:Affected_party': 301, 'B:FE:Sides': 302, 'I:FE:Sides': 303, 'B:FE:Hiding_place': 304, 'I:FE:Hiding_place': 305, 'B:FE:Frequency': 306, 'I:FE:Frequency': 307, 'B:FE:Weapon': 308, 'I:FE:Weapon': 309, 'B:FE:Possibilities': 310, 'I:FE:Possibilities': 311, 'B:FE:Obstruction': 312, 'I:FE:Obstruction': 313, 'B:FE:Skill': 314, 'I:FE:Skill': 315, 'B:FE:Action': 316, 'I:FE:Action': 317, 'B:FE:Direction': 318, 'I:FE:Direction': 319, 'B:FE:Charges': 320, 'I:FE:Charges': 321, 'B:FE:Authorities': 322, 'B:FE:Level': 323, 'B:FE:Qualification': 324, 'B:FE:Mode_of_transportation': 325, 'I:FE:Mode_of_transportation': 326, 'I:FE:Authorities': 327, 'B:FE:Old_leader': 328, 'I:FE:Old_leader': 329, 'B:FE:Transport_means': 330, 'I:FE:Transport_means': 331, 'B:FE:Issue': 332, 'I:FE:Issue': 333, 'I:FE:Carrier': 334, 'B:FE:State': 335, 'I:FE:State': 336, 'B:FE:Event_description': 337, 'I:FE:Event_description': 338, 'B:FE:Speed': 339})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<eos>'] = 0\n",
    "\n",
    "int_labels = []\n",
    "for label in list_labels_1:\n",
    "    int_labels.append([label_vocab[token] for token in label])\n",
    "\n",
    "print(int_labels[12])\n",
    "print(label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoE8LrfcAH_7"
   },
   "source": [
    "Il en va de même pour les textes à qui nous allons dédier un vocabulaire différent de celui des étiquettes. Les embeddings préentraînés seront chargés à partir de ce vocabulaire. Il faut veiller à mettre les mots en minuscules car le vocabulaire des embeddings fasttext est en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6x7qzHAVAJI9",
    "outputId": "b41b941b-d0c4-41c8-f78f-a269d2d6f10f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 115, 12, 34, 116, 12, 117, 118, 119, 120]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18210"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = collections.defaultdict(lambda: len(vocab))\n",
    "vocab['<eos>'] = 0\n",
    "\n",
    "int_texts = []\n",
    "for text in list_text:\n",
    "    int_texts.append([vocab[token.lower()] for token in text])\n",
    "\n",
    "print(int_texts[12])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BgggTg_aCoNt"
   },
   "source": [
    "Egalement nous avons besoin de un input pour indiquer quel est la TARGET. Autrement, s'il y a plusieurs targets dans une phrase, le modèle ne pourrais pas faire la distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "35MXDk5BC6Xk",
    "outputId": "2533080c-3e33-4d9d-d030-56dcf1084c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_indicator_vocab = collections.defaultdict(lambda: len(tg_indicator_vocab))\n",
    "tg_indicator_vocab['<eos>'] = 0\n",
    "\n",
    "int_tg_indicator = []\n",
    "for text in list_tg_or_not:\n",
    "    int_tg_indicator.append([tg_indicator_vocab[token] for token in text])\n",
    "\n",
    "print(int_tg_indicator[12])\n",
    "len(tg_indicator_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mFHhSdZPH2I2",
    "outputId": "609ab0c9-9b8a-4165-a399-045cc8af88ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 3, 4, 7, 14, 7, 14, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab = collections.defaultdict(lambda: len(pos_vocab))\n",
    "pos_vocab['<eos>'] = 0\n",
    "\n",
    "int_pos = []\n",
    "for text in list_pos:\n",
    "    int_pos.append([pos_vocab[token] for token in text])\n",
    "\n",
    "print(int_pos[12])\n",
    "len(pos_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dd0sbSOYAbzO"
   },
   "source": [
    "Nous pouvons maintenant créer des vocabulaires inversés pour pouvoir revenir vers les étiquettes et mots. Celà et utile pour vérifier le contenu d'un tenseur, et surtout pour convertir les décisions du système en étiquettes textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrkoyCKVAfVv"
   },
   "outputs": [],
   "source": [
    "rev_label_vocab = {y: x for x, y in label_vocab.items()}\n",
    "rev_vocab = {y: x for x, y in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAyKpJDSAr6L"
   },
   "source": [
    "Nous pouvons maintenant calculer la longueur moyenne de nos exemples de apprentissage.\n",
    "Ceci nous permettra définir une max_length pertinante pour apprendre notre resseaux\n",
    "(Pas trop de padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "lh6y1xUUBaF5",
    "outputId": "47606a37-bf8c-419f-d1f5-6c629c5cc410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/gabriel/anaconda3/lib/python3.7/site-packages (3.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/gabriel/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/gabriel/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/gabriel/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/gabriel/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.0)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /home/gabriel/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.16.4)\r\n",
      "Requirement already satisfied: six in /home/gabriel/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /home/gabriel/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "GqCVaBkoBB2L",
    "outputId": "e1a45e19-c539-47fd-8862-80c303a51498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.340e+03, 3.541e+03, 1.206e+03, 3.450e+02, 1.180e+02, 1.900e+01,\n",
       "        1.300e+01, 1.700e+01, 3.000e+00, 3.000e+00]),\n",
       " array([  1. ,  21.8,  42.6,  63.4,  84.2, 105. , 125.8, 146.6, 167.4,\n",
       "        188.2, 209. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAASXUlEQVR4nO3df6xc5X3n8fenhtCqiRZTLshrO2s266ohK9Ugr7GU1SpNumCcP0ykjWT+KFaE5K5kpESKqpr2D9Jkkai0CRJSgkSEF1Nlw6ImEVbiXeqlqaL8wQ+TdQzGZX0LNNzYwrc1IYmiZRf63T/msTIxc+8d29dzL37eL2k0Z77nOXOe82j8mbnPnDlOVSFJ6sOvLXUHJEmTY+hLUkcMfUnqiKEvSR0x9CWpI5csdQfmc+WVV9a6deuWuhuS9K7y7LPP/kNVTY1at6xDf926dRw8eHCpuyFJ7ypJ/n6udU7vSFJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjqyYOgn+fUkTyf5YZIjSf6s1R9K8nKSQ+22odWT5L4k00kOJ7l+6Ll2JDnWbjsu3GFJkkYZ5xe5bwIfraqfJ7kU+H6S/97W/VFV/eUZ7W8G1rfbDcD9wA1JrgDuAjYCBTybZF9Vvb4YB7KcrNv9nSXb9yv3fHzJ9i1p+Vvwk34N/Lw9vLTd5vvvtrYBD7ftngQuT7IKuAk4UFWnWtAfALacX/clSWdjrDn9JCuSHAJOMgjup9qqu9sUzr1JLmu11cCrQ5vPtNpc9TP3tTPJwSQHZ2dnz/JwJEnzGSv0q+rtqtoArAE2JfnXwJ3A7wD/BrgC+OPWPKOeYp76mft6oKo2VtXGqamRF4mTJJ2jszp7p6p+AvwNsKWqTrQpnDeB/wJsas1mgLVDm60Bjs9TlyRNyDhn70wlubwt/wbw+8Dftnl6kgS4BXi+bbIPuK2dxbMZeKOqTgCPAzcmWZlkJXBjq0mSJmScs3dWAXuTrGDwJvFoVX07yV8nmWIwbXMI+I+t/X5gKzAN/AL4FEBVnUryBeCZ1u7zVXVq8Q5FkrSQBUO/qg4D142of3SO9gXsmmPdHmDPWfZRkrRI/EWuJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6smDoJ/n1JE8n+WGSI0n+rNWvSfJUkmNJ/luS97T6Ze3xdFu/bui57mz1F5PcdKEOSpI02jif9N8EPlpVvwtsALYk2Qz8OXBvVa0HXgdub+1vB16vqn8F3NvakeRaYDvwIWAL8JUkKxbzYCRJ81sw9Gvg5+3hpe1WwEeBv2z1vcAtbXlbe0xb/7EkafVHqurNqnoZmAY2LcpRSJLGMtacfpIVSQ4BJ4EDwN8BP6mqt1qTGWB1W14NvArQ1r8B/NZwfcQ2w/vameRgkoOzs7Nnf0SSpDmNFfpV9XZVbQDWMPh0/sFRzdp95lg3V/3MfT1QVRurauPU1NQ43ZMkjemszt6pqp8AfwNsBi5PcklbtQY43pZngLUAbf0/A04N10dsI0magHHO3plKcnlb/g3g94GjwHeB/9Ca7QAea8v72mPa+r+uqmr17e3snmuA9cDTi3UgkqSFXbJwE1YBe9uZNr8GPFpV307yAvBIkv8E/C/gwdb+QeAvkkwz+IS/HaCqjiR5FHgBeAvYVVVvL+7hSJLms2DoV9Vh4LoR9ZcYcfZNVf0f4JNzPNfdwN1n301J0mLwF7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIgqGfZG2S7yY5muRIkk+3+ueS/DjJoXbbOrTNnUmmk7yY5Kah+pZWm06y+8IckiRpLpeM0eYt4LNV9YMk7wOeTXKgrbu3qv7zcOMk1wLbgQ8B/xz4n0l+u63+MvDvgRngmST7quqFxTgQSdLCFgz9qjoBnGjLP0tyFFg9zybbgEeq6k3g5STTwKa2brqqXgJI8khra+hL0oSc1Zx+knXAdcBTrXRHksNJ9iRZ2WqrgVeHNptptbnqkqQJGTv0k7wX+Abwmar6KXA/8AFgA4O/BL54uumIzWue+pn72ZnkYJKDs7Oz43ZPkjSGsUI/yaUMAv9rVfVNgKp6rarerqp/Ar7KL6dwZoC1Q5uvAY7PU/8VVfVAVW2sqo1TU1NnezySpHmMc/ZOgAeBo1X1paH6qqFmnwCeb8v7gO1JLktyDbAeeBp4Blif5Jok72HwZe++xTkMSdI4xjl758PAHwDPJTnUan8C3JpkA4MpmleAPwSoqiNJHmXwBe1bwK6qehsgyR3A48AKYE9VHVnEY5EkLWCcs3e+z+j5+P3zbHM3cPeI+v75tpMkXVj+IleSOmLoS1JHDH1J6oihL0kdGefsnXetdbu/s9RdkKRlxU/6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOLBj6SdYm+W6So0mOJPl0q1+R5ECSY+1+ZasnyX1JppMcTnL90HPtaO2PJdlx4Q5LkjTKOJ/03wI+W1UfBDYDu5JcC+wGnqiq9cAT7THAzcD6dtsJ3A+DNwngLuAGYBNw1+k3CknSZCwY+lV1oqp+0JZ/BhwFVgPbgL2t2V7glra8DXi4Bp4ELk+yCrgJOFBVp6rqdeAAsGVRj0aSNK+zmtNPsg64DngKuLqqTsDgjQG4qjVbDbw6tNlMq81VP3MfO5McTHJwdnb2bLonSVrA2KGf5L3AN4DPVNVP52s6olbz1H+1UPVAVW2sqo1TU1Pjdk+SNIaxQj/JpQwC/2tV9c1Wfq1N29DuT7b6DLB2aPM1wPF56pKkCRnn7J0ADwJHq+pLQ6v2AafPwNkBPDZUv62dxbMZeKNN/zwO3JhkZfsC98ZWkyRNyCVjtPkw8AfAc0kOtdqfAPcAjya5HfgR8Mm2bj+wFZgGfgF8CqCqTiX5AvBMa/f5qjq1KEchSRrLgqFfVd9n9Hw8wMdGtC9g1xzPtQfYczYdlCQtHn+RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjiwY+kn2JDmZ5Pmh2ueS/DjJoXbbOrTuziTTSV5MctNQfUurTSfZvfiHIklayDif9B8Ctoyo31tVG9ptP0CSa4HtwIfaNl9JsiLJCuDLwM3AtcCtra0kaYIuWahBVX0vyboxn28b8EhVvQm8nGQa2NTWTVfVSwBJHmltXzjrHkuSztn5zOnfkeRwm/5Z2WqrgVeH2sy02lz1d0iyM8nBJAdnZ2fPo3uSpDOda+jfD3wA2ACcAL7Y6hnRtuapv7NY9UBVbayqjVNTU+fYPUnSKAtO74xSVa+dXk7yVeDb7eEMsHao6RrgeFueqy5JmpBz+qSfZNXQw08Ap8/s2QdsT3JZkmuA9cDTwDPA+iTXJHkPgy979517tyVJ52LBT/pJvg58BLgyyQxwF/CRJBsYTNG8AvwhQFUdSfIogy9o3wJ2VdXb7XnuAB4HVgB7qurIoh+NJGle45y9c+uI8oPztL8buHtEfT+w/6x6J0laVP4iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sg5XVpZy9e63d9Zkv2+cs/Hl2S/ks6On/QlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHVkw9JPsSXIyyfNDtSuSHEhyrN2vbPUkuS/JdJLDSa4f2mZHa38syY4LcziSpPmM80n/IWDLGbXdwBNVtR54oj0GuBlY3247gfth8CYB3AXcAGwC7jr9RiFJmpwFQ7+qvgecOqO8DdjblvcCtwzVH66BJ4HLk6wCbgIOVNWpqnodOMA730gkSRfYuc7pX11VJwDa/VWtvhp4dajdTKvNVX+HJDuTHExycHZ29hy7J0kaZbG/yM2IWs1Tf2ex6oGq2lhVG6empha1c5LUu3MN/dfatA3t/mSrzwBrh9qtAY7PU5ckTdC5hv4+4PQZODuAx4bqt7WzeDYDb7Tpn8eBG5OsbF/g3thqkqQJWvB6+km+DnwEuDLJDIOzcO4BHk1yO/Aj4JOt+X5gKzAN/AL4FEBVnUryBeCZ1u7zVXXml8OSpAtswdCvqlvnWPWxEW0L2DXH8+wB9pxV7yRJi8pf5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSPnFfpJXknyXJJDSQ622hVJDiQ51u5XtnqS3JdkOsnhJNcvxgFIksa3GJ/0f6+qNlTVxvZ4N/BEVa0HnmiPAW4G1rfbTuD+Rdi3JOksXIjpnW3A3ra8F7hlqP5wDTwJXJ5k1QXYvyRpDucb+gX8VZJnk+xstaur6gRAu7+q1VcDrw5tO9NqkqQJueQ8t/9wVR1PchVwIMnfztM2I2r1jkaDN4+dAO9///vPs3uSpGHn9Um/qo63+5PAt4BNwGunp23a/cnWfAZYO7T5GuD4iOd8oKo2VtXGqamp8+meJOkM5xz6SX4zyftOLwM3As8D+4AdrdkO4LG2vA+4rZ3Fsxl44/Q0kCRpMs5neudq4FtJTj/Pf62q/5HkGeDRJLcDPwI+2drvB7YC08AvgE+dx74lSefgnEO/ql4CfndE/R+Bj42oF7DrXPcnSTp//iJXkjpi6EtSRwx9SeqIoS9JHTnfH2dJAKzb/Z0l2e8r93x8SfYrvVv5SV+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHfGCa3pXW6oLvYEXe9O7k5/0Jakjhr4kdcTQl6SOTDz0k2xJ8mKS6SS7J71/SerZREM/yQrgy8DNwLXArUmunWQfJKlnkz57ZxMwXVUvASR5BNgGvDDhfkjnbSnPHOqNZ0otnkmH/mrg1aHHM8ANww2S7AR2toc/T/LiOeznSuAfzqmH/XCMxuM4LeyCj1H+/EI++0RM+nX0L+ZaMenQz4ha/cqDqgeAB85rJ8nBqtp4Ps9xsXOMxuM4LcwxWthyGqNJf5E7A6wderwGOD7hPkhStyYd+s8A65Nck+Q9wHZg34T7IEndmuj0TlW9leQO4HFgBbCnqo5cgF2d1/RQJxyj8ThOC3OMFrZsxihVtXArSdJFwV/kSlJHDH1J6shFF/pe5mG0JK8keS7JoSQHW+2KJAeSHGv3K5e6n5OUZE+Sk0meH6qNHJMM3NdeV4eTXL90PZ+cOcboc0l+3F5Lh5JsHVp3ZxujF5PctDS9nqwka5N8N8nRJEeSfLrVl+Vr6aIKfS/zsKDfq6oNQ+cL7waeqKr1wBPtcU8eAracUZtrTG4G1rfbTuD+CfVxqT3EO8cI4N72WtpQVfsB2r+17cCH2jZfaf8mL3ZvAZ+tqg8Cm4FdbSyW5Wvpogp9hi7zUFX/Fzh9mQeNtg3Y25b3ArcsYV8mrqq+B5w6ozzXmGwDHq6BJ4HLk6yaTE+XzhxjNJdtwCNV9WZVvQxMM/g3eVGrqhNV9YO2/DPgKIOrDyzL19LFFvqjLvOweon6stwU8FdJnm2XugC4uqpOwOCFC1y1ZL1bPuYaE19bv+qONjWxZ2hasPsxSrIOuA54imX6WrrYQn/Byzx07MNVdT2DPy13Jfl3S92hdxlfW790P/ABYANwAvhiq3c9RkneC3wD+ExV/XS+piNqExuniy30vczDHKrqeLs/CXyLwZ/dr53+s7Ldn1y6Hi4bc42Jr62mql6rqrer6p+Ar/LLKZxuxyjJpQwC/2tV9c1WXpavpYst9L3MwwhJfjPJ+04vAzcCzzMYmx2t2Q7gsaXp4bIy15jsA25rZ15sBt44/ad7b86Yf/4Eg9cSDMZoe5LLklzD4IvKpyfdv0lLEuBB4GhVfWlo1fJ8LVXVRXUDtgL/G/g74E+Xuj/L4Qb8S+CH7Xbk9LgAv8XgrIJj7f6Kpe7rhMfl6wymJ/4fg09ft881Jgz+JP9ye109B2xc6v4v4Rj9RRuDwwwCbNVQ+z9tY/QicPNS939CY/RvGUzPHAYOtdvW5fpa8jIMktSRi216R5I0D0Nfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdeT/A3VlIvWEjWTrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lengths = []\n",
    "len_texts = [ len(text) for text in list_text ]\n",
    "\n",
    "plt.hist(len_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwlE_sKpAmt3"
   },
   "source": [
    "Importons les modules habituels de pytorch. Les constantes suivantes sont définies :\n",
    "\n",
    "    max_len est la longueur maximale d'une phrase en apprentissage\n",
    "    batch_size est la taille des batches\n",
    "    embed_size est la taille des embeddings préentraînés (nous utiliserons les embeddings téléchargeables sur le site de fasttext qui sont de taille 300.\n",
    "    hidden_size est la taille de l'état caché du RNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_1d6ab1Aok_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "max_len = 50\n",
    "batch_size = 64\n",
    "embed_size = 300\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcXbWU5EB6Xm"
   },
   "source": [
    "Il faut maintenant créer des tenseurs pytorch avec les données phrases et les étiquettes associées. X et Y sont des tenseurs d'entiers de taille (le nombre d'exemples du corpus, la longueur maximale d'une phrase). Ils sont initialisés à zéro car c'est la valeur de padding.\n",
    "\n",
    "Pour chaque phrase et séquence d'étiquettes associées, nous calculons la longueur effective à intégrer en fonction de la longueur maximale (les phrases et séquences d'étiquettes trop longues sont coupées). Puis nous pouvons les intégrer dans les tenseurs en début de ligne, les fins de lignes étant remplies de zéros (`<eos>`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "t3860ZZsB9_F",
    "outputId": "f3adda63-d7e8-4a26-f9a8-ec0cca7dbbab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 23, 115,  12,  34, 116,  12, 117, 118, 119, 120,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0])\n",
      "tensor([1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([ 3,  4,  7,  3,  4,  7, 14,  7, 14,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "tensor([ 1, 11, 12, 13, 13,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "X = torch.zeros(len(int_texts), max_len).long()\n",
    "T = torch.zeros(len(int_texts), max_len).long()\n",
    "P = torch.zeros(len(int_texts), max_len).long()\n",
    "Y = torch.zeros(len(int_labels), max_len).long()\n",
    "\n",
    "for i, (text, tg, pos, label) in enumerate(zip(int_texts, int_tg_indicator, int_pos, int_labels)):\n",
    "    length = min(max_len, len(text))\n",
    "    X[i,:length] = torch.LongTensor(text[:length])\n",
    "    T[i,:length] = torch.LongTensor(tg[:length])\n",
    "    P[i,:length] = torch.LongTensor(pos[:length])\n",
    "    Y[i,:length] = torch.LongTensor(label[:length])\n",
    "\n",
    "print(X[12])\n",
    "print(T[12])\n",
    "print(P[12])\n",
    "print(Y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKGbpAoCD1cX"
   },
   "source": [
    "Pour vérifier les performances du système, nous le séparons en un ensemble d'entraînement et un ensemble de développement. Comme il y a 7605 exemple, nous utiliserons les 6000 premiers pour l'entraînement et les 1605 suivants pour la validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qrul-szD2l_"
   },
   "outputs": [],
   "source": [
    "X_train = X[:6000]\n",
    "T_train = T[:6000]\n",
    "P_train = P[:6000]\n",
    "Y_train = Y[:6000]\n",
    "\n",
    "X_valid = X[6000:]\n",
    "T_valid = T[6000:]\n",
    "P_valid = P[6000:]\n",
    "Y_valid = Y[6000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1nPzhSsERT9"
   },
   "source": [
    "Torch contient des classes permettant facilement de charger des batches d'exemples déjà mélangés pour l'entraînement. Nous allons en tirer parti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KS59yoxXH7Fo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_set = TensorDataset(X_train, T_train, P_train, Y_train)\n",
    "valid_set = TensorDataset(X_valid, T_valid, P_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tRT9d8zeINHs"
   },
   "source": [
    "La prochaine étape consiste en le chargement des embeddings préentraînés. Les embeddings fasttext peuvent être téléchargés depuis la page suivante : \n",
    "\n",
    "https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "Toutefois, ces embeddings couvrant plusieurs millions de mots, le fichier fait plusieurs GiB et une version filtrée par rapport au vocabulaire du corpus est disponible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKicuq4OIXA3"
   },
   "outputs": [],
   "source": [
    "![ -f wiki.fr.vec.small ] || wget -q http://pageperso.lif.univ-mrs.fr/gabriel.marzinotto/wiki.fr.vec.small "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T63VYPaoJ99X"
   },
   "source": [
    "Les embeddings ont le format suivant :\n",
    "\n",
    "    chaque ligne décrit l'embedding d'un mot\n",
    "    le mot est sur la première colone, suivi des valeurs du vecteur dans l'espace de 300 dimension.\n",
    "\n",
    "Nous créons donc un tenseur à zéro, puis plaçons l'embedding de chaque mot du vocabulaire rencontré à la bonne ligne dans le tenseur. Les embeddings des mots non couverts dans ce fichier resteront à zéro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cfbXSs4aKOty",
    "outputId": "f45cef04-0360-4914-c1ea-a24aa3a2b920"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0988, -0.1492, -0.0365,  0.0521,  0.2108, -0.0527,  0.0233,  0.0760,\n",
       "         0.1252,  0.2180])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights = torch.zeros(len(vocab), 300)\n",
    "with open('wiki.fr.vec.small') as fp:\n",
    "    for line in fp:\n",
    "        tokens = line.strip().split()\n",
    "        if tokens[0] in vocab:\n",
    "            try:\n",
    "              pretrained_weights[vocab[tokens[0]]] = torch.FloatTensor([float(x) for x in tokens[1:]])\n",
    "            except:\n",
    "              continue\n",
    "                \n",
    "pretrained_weights[12][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhA7mFgWKtZ2"
   },
   "source": [
    "Le modèle est une class qui étend nn.Module. Son constructeur appelle le constructeur de la class mère, puis déclare les différentes couches : une couche d'embeddings, une couche de GRU et une couche de décision. Cette couche de décision sera appliquée à chaque position de la phrase pour prédire l'étiquette associée.\n",
    "\n",
    "Le premier détail important que nous allons traiter est le problème du padding. Pour pouvoir prendre en entrée des batches de phrases de la même taille, nous avons complété ces dernières avec des 0. Le mot d'indice zéro a un embedding et le système peut donc l'utiliser pour apprendre des régularités des données. Si on utilise un RNN bidirectionnel, l'état caché après chaque padding peut aussi contribuer à la représentation créée. Ces deux problème font que le modèle aura un comportement différent si on change la taille des séquences traitées.\n",
    "\n",
    "Une solution est de forcer l'état caché à rester à zéro en présence de padding. Pour celà il faut spécifier le padding_idx de la couche d'emebdding pour la représentation associée au padding soit toujours le vecteur nul. Ensuite, dans le RNN, l'état caché est calculé comme une transformation affine à partir de l'embedding en entrée et de l'état caché précédent. Comme l'état caché initial est à zéro, et que l'embedding du padding est à zéro, si on désactive le bias dans les calculs (en donnant l'option bias=False au GRU), cela va forcer l'état caché à rester à zéro tout au long du padding. Le modèle peut ainsi traiter des séquences de taille arbitraire, même quand il est bidirectionnel.\n",
    "\n",
    "Le deuxième détail est l'initialisation des embeddings. La méthode diffère selon les versions de pytorch, mais la façon présentée ici marche dans la plupart des cas. Nous allons directement modifier le champ weight de la couche d'embeddings et remplacer ce paramètre par nos embeddings préentraînés. requires_grad=False permet de geler la couche d'embeddings et donc de ne pas les modifier lors de l'apprentissage. Cela permettra en prédiction d'utiliser des embeddings fasttext pour les mots que nous n'avons pas observés dans le corpus d'apprentissage, en espérant qu'un mot d'embedding proche s'y trouvait. Si on omet cette option, les embeddings sont fine-tunés pour maximiser les performances du système.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "4GLvRktvJ7zx",
    "outputId": "ec796dd2-32bb-4ace-bf7c-b5dffd95118e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 340])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
    "        self.word_embed.weight = nn.Parameter(pretrained_weights, requires_grad=False)\n",
    "        \n",
    "        #Initialize the embeddings for the TARGET_OR_NOT using the identity matrix\n",
    "        tg_ind = len(tg_indicator_vocab)\n",
    "        self.tg_embed = nn.Embedding(tg_ind, tg_ind, padding_idx=tg_indicator_vocab['<eos>'])\n",
    "        self.tg_embed.weight = nn.Parameter(torch.eye(tg_ind), requires_grad=False)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size+tg_ind, hidden_size, bias=False, num_layers=1, dropout=0.3, bidirectional=False, batch_first=True)\n",
    "        self.decision = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        word_embed = self.word_embed(x)\n",
    "        tg_embed = self.tg_embed(t)\n",
    "        \n",
    "        embed = torch.cat((word_embed,tg_embed),-1)\n",
    "        \n",
    "        output, hidden = self.rnn(embed)\n",
    "        return self.decision(output)\n",
    "\n",
    "rnn_model = RNN()\n",
    "rnn_model\n",
    "\n",
    "rnn_model(Variable(X[:2]), Variable(T[:2])).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aqwp_p6Pqmi"
   },
   "source": [
    "La fonction qui calcule les performances d'un modèle doit accomoder la nouvelle forme des données.\n",
    "En particulier, le critère d'entropie croisée n'accepte que des matrices à deux dimensions, donc il faut redimentionner les scores produits pour qu'ils ait la taille `(batch_size * sequence_length, num_labels)` et les références pour quelles aient la taille `(batch_size * sequence_length)`.\n",
    "\n",
    "Ensuite, il faut modifier le max qui calcule les prédictions pour qu'il agisse sur la dernière dimension du tenseur `y_scores`.\n",
    "\n",
    "Et finalement, pour calculer le score d'une séquence, nous devons ignorer le padding. Pour celà une matrice `mask` est crée qui contient 1 pour les éléments non nuls de la matrice contenant les étiquettes et 0 sinon.\n",
    "On peut calculer le nombre de corrects ainsi que le numérateur de la fonction de performance en appliquant le masque.\n",
    "\n",
    "Le loss, pour être comparable au loss de l'entraînement, n'est pas modifié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "NLcMFWOLPoXz",
    "outputId": "1b579d21-1ccc-48a7-c5cc-0b343b2b44dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0936), 0.0010965732087227413)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perf(model, loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total_loss = correct = num_loss = num_perf = 0\n",
    "    for x, t, p, y in loader:\n",
    "        y_scores = model(Variable(x, volatile=True), Variable(t, volatile=True))\n",
    "        loss = criterion( y_scores.view(y.size(0) * y.size(1), -1), \n",
    "                          Variable(y.view(y.size(0) * y.size(1)), volatile=True) )\n",
    "        y_pred = torch.max(y_scores, 2)[1]\n",
    "        mask = (y != 0)\n",
    "        correct += torch.sum((y_pred.data == y) * mask)\n",
    "        total_loss += loss.data\n",
    "        num_loss += len(y)\n",
    "        num_perf += torch.sum(mask)\n",
    "        \n",
    "    #print(\"df\", correct, num_perf)\n",
    "    return total_loss / num_loss, float(correct) / float(num_perf)\n",
    "\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGQmfx-JQxMI"
   },
   "source": [
    "La fonction d'apprentissage est modifiée en deux endroits. Tout d'abord, pytorch refuse d'entraîner un modèle contenant des paramètres sans gradient. Nous devons donc filtrer la liste des paramètres passés à l'optimiseur pour qu'elle ne contienne pas les embeddings \"gelés\".\n",
    "\n",
    "Ensuite, nous appliquons le même redimenssionnement des scores et des équeittes pour accomoder le critère d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "-yyVjEAFQ3yY",
    "outputId": "6d9d86da-bf57-4357-fe39-6dbb034dba50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0334) (tensor(0.0191), 0.6725482866043614)\n",
      "1 tensor(0.0203) (tensor(0.0182), 0.6747663551401869)\n",
      "2 tensor(0.0194) (tensor(0.0175), 0.6756884735202492)\n",
      "3 tensor(0.0183) (tensor(0.0163), 0.6779314641744548)\n",
      "4 tensor(0.0170) (tensor(0.0153), 0.6891214953271028)\n",
      "5 tensor(0.0159) (tensor(0.0148), 0.6973707165109034)\n",
      "6 tensor(0.0151) (tensor(0.0140), 0.7037507788161994)\n",
      "7 tensor(0.0144) (tensor(0.0137), 0.7057943925233645)\n",
      "8 tensor(0.0136) (tensor(0.0130), 0.7137943925233645)\n",
      "9 tensor(0.0130) (tensor(0.0125), 0.7171588785046729)\n",
      "10 tensor(0.0124) (tensor(0.0122), 0.7210716510903427)\n",
      "11 tensor(0.0119) (tensor(0.0118), 0.7247850467289719)\n",
      "12 tensor(0.0114) (tensor(0.0115), 0.7283738317757009)\n",
      "13 tensor(0.0110) (tensor(0.0113), 0.7331339563862929)\n",
      "14 tensor(0.0106) (tensor(0.0111), 0.7358753894080997)\n",
      "15 tensor(0.0102) (tensor(0.0109), 0.7348785046728972)\n",
      "16 tensor(0.0099) (tensor(0.0107), 0.7382928348909658)\n",
      "17 tensor(0.0095) (tensor(0.0106), 0.7416074766355141)\n",
      "18 tensor(0.0092) (tensor(0.0105), 0.7441495327102804)\n",
      "19 tensor(0.0090) (tensor(0.0104), 0.7400872274143302)\n"
     ]
    }
   ],
   "source": [
    "def fit(model, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = num = 0\n",
    "        for x, t, p, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_scores = model(Variable(x), Variable(t))\n",
    "            loss = criterion(y_scores.view(y.size(0) * y.size(1), -1), Variable(y.view(y.size(0) * y.size(1)), volatile=True))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num += len(y)\n",
    "        print(epoch, total_loss / num, perf(model, valid_loader))\n",
    "\n",
    "fit(rnn_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIlR7n8_S0XQ"
   },
   "source": [
    "Une fois le modèle entraîné, nous pouvons écrire une fonction qui génère les prédictions pour une phrase. Cette fonction devra :\n",
    "\n",
    "    convertir les mots en entiers (nous n'avons pas prévu de symbole pour les mots inconnus donc ils seront remplacés par du padding)\n",
    "    faire un batch de taille 1 sous la forme d'un tenseur de dimensions (1, longueur de la phrase)\n",
    "    calculer les scores des étiquettes à chaque position\n",
    "    calculer les prédictions correspondantes\n",
    "    et convertir les entiers prédits en text grâce au dictionnaire inversé créé plus haut\n",
    "\n",
    "Notez que cette fonction peut prendre en entrée des phrases plus longues que max_len et que le temps de calcul n'est pas gaspillé par le padding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMyregzMZh1d"
   },
   "source": [
    "## Viterbi Decoding\n",
    "L'idée est de pouvoir appliquer certains contraintes sur les sorties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "tIx-ntN7d1dN",
    "outputId": "76550873-b972-4aab-8e0a-a6445c86514e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 340])\n",
      "['<eos>', 'O', 'B:FE:Product', 'I:FE:Product', 'B:TG:Subjective_influence', 'B:FE:Entity', 'I:FE:Entity', 'B:FE:Domain', 'I:FE:Domain', 'B:FE:Cognizer', 'I:FE:Cognizer', 'B:TG:Activity_start', 'B:FE:Activity', 'I:FE:Activity']\n",
      "tensor([0., 0., 0., 0., 0., 0., -inf, 0., -inf, 0., -inf, 0., 0., -inf])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def get_transition_params(label_strs):\n",
    "  '''Construct transtion scoresd (0 for allowed, -inf for invalid).\n",
    "  Args:\n",
    "    label_strs: A [num_tags,] sequence of BIO-tags.\n",
    "  Returns:\n",
    "    A [num_tags, num_tags] matrix of transition scores.  \n",
    "  '''\n",
    "  num_tags = len(label_strs)\n",
    "  transition_params = numpy.zeros([num_tags, num_tags], dtype=numpy.float32)\n",
    "  for i, prev_label in enumerate(label_strs):\n",
    "    for j, label in enumerate(label_strs):\n",
    "      if i != j and label[0] == 'I' and not prev_label == 'B' + label[1:]:\n",
    "        transition_params[i,j] = numpy.NINF\n",
    "  return torch.tensor(transition_params)\n",
    "\n",
    "\n",
    "\n",
    "label_strs = [ rev_label_vocab[i] for i in range(len(rev_label_vocab)) ]\n",
    "transition_params = get_transition_params(label_strs)\n",
    "\n",
    "\n",
    "print(transition_params.shape)\n",
    "#check the transition penalities from B:FE:Product for 15 labels\n",
    "print(label_strs[0:14])\n",
    "print(transition_params[2,0:14]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "i_BJTbAtTMap",
    "outputId": "e8496b46-4681-45cb-eb88-ead3887777a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:122: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.44782499371385465, 0.4317575757575758, 0.4396445322142681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:128: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6385916465308941, 0.4484848484848485, 0.5269154087154657)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def viterbi_decode(score, transition_params):\n",
    "  \"\"\" Adapted from Tensorflow implementation.\n",
    "  Decode the highest scoring sequence of tags outside of TensorFlow.\n",
    "  This should only be used at test time.\n",
    "  Args:\n",
    "    score: A [seq_len, num_tags] matrix of unary potentials.\n",
    "    transition_params: A [num_tags, num_tags] matrix of binary potentials.\n",
    "  Returns:\n",
    "    viterbi: A [seq_len] list of integers containing the highest scoring tag\n",
    "        indicies.\n",
    "    viterbi_score: A float containing the score for the Viterbi sequence.\n",
    "  \"\"\"\n",
    "  trellis = torch.zeros_like(score)\n",
    "  backpointers = torch.zeros_like(score)\n",
    "  trellis[0] = score[0]\n",
    "  for t in range(1, score.shape[0]):\n",
    "    v = trellis[t - 1].unsqueeze(1) + transition_params\n",
    "    trellis[t] = score[t] + torch.max(v, 0)[0]\n",
    "    backpointers[t] = torch.argmax(v, 0)\n",
    "  viterbi = [torch.argmax(trellis[-1]).long().view(1)]\n",
    "  for bp in reversed(backpointers[1:]):\n",
    "    viterbi.append(bp[viterbi[-1]].long().view(1))\n",
    "  viterbi.reverse()\n",
    "  viterbi = torch.cat(viterbi,0)\n",
    "  viterbi_score, index = torch.max(trellis[-1],-1)\n",
    "  \n",
    "  return viterbi, viterbi_score\n",
    "\n",
    "# we will use rev_label_vocab to go from ids -> labels\n",
    "def fscore(model, loader, viterbi=None):\n",
    "  \n",
    "    def build_arg_matrix(y_tensor):\n",
    "        refs = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict()) )\n",
    "        nb_refs = 0.0\n",
    "        \n",
    "        batch_size, length = y_tensor.shape\n",
    "\n",
    "        for i_ in range(batch_size):\n",
    "          y_sent = y_tensor[i_,:]\n",
    "          start_elem = None\n",
    "          end_elem   = None\n",
    "          label_elem = None\n",
    "\n",
    "          for j,y in enumerate(y_sent):\n",
    "  \n",
    "            label = rev_label_vocab[y]\n",
    "            # if padding...\n",
    "            if(y == '<eos>'): \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "\n",
    "              start_elem = None\n",
    "              end_elem   = None\n",
    "              label_elem = None\n",
    "            \n",
    "            \n",
    "            elif(label[0] == \"B\"):\n",
    "              \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "                \n",
    "              start_elem = j\n",
    "              end_elem   = j\n",
    "              label_elem = label[2:]\n",
    "              \n",
    "              \n",
    "            elif(label[0] == \"I\"):\n",
    "                if(label[2:] == label_elem):\n",
    "                  end_elem = j\n",
    "                else:\n",
    "                  if( start_elem and end_elem and label_elem):\n",
    "                    refs[i_][start_elem][end_elem] = label_elem   \n",
    "                    nb_refs += 1\n",
    "              \n",
    "                  start_elem = j\n",
    "                  end_elem   = j\n",
    "                  label_elem = label[2:]              \n",
    "                              \n",
    "            elif(label[0] == \"O\"):              \n",
    "              if( start_elem and end_elem and label_elem):\n",
    "                refs[i_][start_elem][end_elem] = label_elem\n",
    "                nb_refs += 1\n",
    "\n",
    "              start_elem = None\n",
    "              end_elem   = None\n",
    "              label_elem = None\n",
    "          \n",
    "          \n",
    "          if( start_elem and end_elem and label_elem):\n",
    "            refs[i_][start_elem][end_elem] = label_elem\n",
    "            nb_refs += 1\n",
    "\n",
    "        return refs, nb_refs\n",
    "  \n",
    "  \n",
    "    def compare(y_true, y_pred):\n",
    "        refs, nb_refs = build_arg_matrix(y_true.numpy())\n",
    "        hyps, nb_hyps = build_arg_matrix(y_pred.numpy())\n",
    "                \n",
    "        nb_ok   = 0.0\n",
    "        counter = 0\n",
    "        for i, dico in refs.items():\n",
    "          for j, subdico in dico.items():\n",
    "            for k, value in subdico.items():   \n",
    "              if( i in hyps and\n",
    "                  j in hyps[i] and \n",
    "                  k in hyps[i][j] and\n",
    "                  hyps[i][j][k] == value ):\n",
    "                nb_ok += 1.0   \n",
    "              \n",
    "        return nb_ok, nb_refs, nb_hyps\n",
    "\n",
    "    nb_ok = nb_ref = nb_hyp = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for x, t, p, y in loader:\n",
    "        y_scores = model(Variable(x, volatile=True), Variable(t, volatile=True))\n",
    "        \n",
    "        if(viterbi is not None):\n",
    "          bs, tm, _ = y_scores.shape\n",
    "          y_pred = torch.zeros((bs, tm))\n",
    "          for i, y_score in enumerate(y_scores):\n",
    "            y_score = torch.log( F.softmax(y_score) )\n",
    "            a_pred, _ = viterbi_decode(y_score, viterbi)\n",
    "            y_pred[i] = a_pred\n",
    "        else:\n",
    "          y_pred = torch.max(y_scores, 2)[1]\n",
    "\n",
    "        k, r, h = compare(y, y_pred)\n",
    "        nb_ok  += k\n",
    "        nb_ref += r\n",
    "        nb_hyp += h\n",
    "    \n",
    "    prec = nb_ok/nb_hyp\n",
    "    recall = nb_ok/nb_ref\n",
    "    f1 = 2*(prec*recall)/(prec+recall)\n",
    "    \n",
    "    return prec, recall, f1\n",
    "\n",
    "print( fscore(rnn_model, valid_loader) )\n",
    "print( fscore(rnn_model, valid_loader, viterbi=transition_params) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "Zlcta3yo4B8a",
    "outputId": "c18a0d9b-875c-4497-d2f8-25239fe5cac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 340])\n",
      "<eos> tensor(-6.2729)\n",
      "O tensor(-6.2729)\n",
      "B:FE:Product tensor(-6.2729)\n",
      "I:FE:Product tensor(-1.0206)\n",
      "B:TG:Subjective_influence tensor(-6.2729)\n",
      "B:FE:Entity tensor(-6.2729)\n",
      "I:FE:Entity tensor(-inf)\n",
      "B:FE:Domain tensor(-6.2729)\n",
      "I:FE:Domain tensor(-inf)\n",
      "B:FE:Cognizer tensor(-6.2729)\n",
      "I:FE:Cognizer tensor(-inf)\n",
      "B:TG:Activity_start tensor(-6.2729)\n",
      "B:FE:Activity tensor(-6.2729)\n",
      "I:FE:Activity tensor(-inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:122: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/gabriel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:128: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7503401360544217, 0.2673939393939394, 0.3942806076854334)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_transition_params_from_data(label_strs, y_train):\n",
    "  '''Construct transtion scoresd (0 for allowed, -inf for invalid).\n",
    "  Args:\n",
    "    label_strs: A [num_tags,] sequence of BIO-tags.\n",
    "  Returns:\n",
    "    A [num_tags, num_tags] matrix of transition scores.  \n",
    "  '''\n",
    "  num_tags = len(label_strs)\n",
    "  transition_params = numpy.zeros([num_tags, num_tags], dtype=numpy.float32) \n",
    "  for i, prev_label in enumerate(label_strs):\n",
    "    for j, label in enumerate(label_strs):\n",
    "      if i != j and label[0] == 'I' and not prev_label == 'B' + label[1:]:\n",
    "        transition_params[i,j] = numpy.NINF\n",
    "  \n",
    "  transition_counts = numpy.zeros([num_tags, num_tags], dtype=numpy.float32) + 0.1\n",
    "  for seq in y_train:\n",
    "    for i in range(len(seq)-1):\n",
    "      prev_label = seq[i]\n",
    "      next_lavel = seq[i+1]\n",
    "      transition_counts[prev_label,next_lavel] += 1\n",
    "      \n",
    "  row_sums = transition_counts.sum(axis=1, keepdims=True)\n",
    "  loglikelihoods = numpy.log( transition_counts  / row_sums )\n",
    "  \n",
    "  transition_params = transition_params + loglikelihoods\n",
    "  \n",
    "  return torch.tensor(transition_params)\n",
    "\n",
    "\n",
    "\n",
    "label_strs = [ rev_label_vocab[i] for i in range(len(rev_label_vocab)) ]\n",
    "transition_params = get_transition_params_from_data(label_strs, Y_train)\n",
    "\n",
    "\n",
    "print(transition_params.shape)\n",
    "#check the transition penalities from B:FE:Product for 15 labels\n",
    "for l,c in zip( label_strs[0:14], transition_params[2,0:14] ):\n",
    "  print(l,c)\n",
    "\n",
    "print( fscore(rnn_model, valid_loader, viterbi=transition_params) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPreBEqKZRnQ"
   },
   "source": [
    "##RNN-CRF\n",
    "Nous avons la possibilité de faire un modèle CRF sur les features calculés par notre RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMT0gk0rfjo1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(159.4783, grad_fn=<SelectBackward>), tensor(159.4845, grad_fn=<SelectBackward>)]\n",
      "[[138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138], [138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138, 138]]\n"
     ]
    }
   ],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "  \n",
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embed = nn.Embedding(len(vocab), embed_size, padding_idx=vocab['<eos>'])\n",
    "        self.word_embed.weight = nn.Parameter(pretrained_weights, requires_grad=False)\n",
    "        \n",
    "        #Initialize the embeddings for the TARGET_OR_NOT using the identity matrix\n",
    "        tg_ind = len(tg_indicator_vocab)\n",
    "        self.tg_embed = nn.Embedding(tg_ind, tg_ind, padding_idx=tg_indicator_vocab['<eos>'])\n",
    "        self.tg_embed.weight = nn.Parameter(torch.eye(tg_ind), requires_grad=False)\n",
    "        \n",
    "        self.tagset_size = len(label_vocab)\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_size + tg_ind, hidden_size, bias=False, num_layers=1, dropout=0.3, bidirectional=False, batch_first=True)\n",
    "        self.decision_frame = nn.Linear(hidden_size * 1 * 1, len(label_vocab))\n",
    "        \n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.log(torch.full((1, self.tagset_size), 1.0/float(self.tagset_size)))\n",
    "        end_vvars   = torch.log(torch.full((1, self.tagset_size), 1.0/float(self.tagset_size)))\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + end_vvars\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "      \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        \n",
    "        for i, feat in enumerate(feats[0:-1]):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        #score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "      \n",
    "      \n",
    "    def _viterbi_decode(self, feats):\n",
    "        \n",
    "        all_scores   = []\n",
    "        all_bestpath = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.log(torch.full((1, self.tagset_size), 1.0/float(self.tagset_size)))\n",
    "        end_vvars  = torch.log(torch.full((1, self.tagset_size), 1.0/float(self.tagset_size)))\n",
    "        \n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        for batch_elem in feats: \n",
    "            backpointers = []\n",
    "            forward_var = init_vvars\n",
    "            for feat in batch_elem:\n",
    "                bptrs_t = []  # holds the backpointers for this step\n",
    "                viterbivars_t = []  # holds the viterbi variables for this step\n",
    "                #print(\"feat\",feat.shape)\n",
    "                for next_tag in range(self.tagset_size):\n",
    "                    # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                    # previous step, plus the score of transitioning\n",
    "                    # from tag i to next_tag.\n",
    "                    # We don't include the emission scores here because the max\n",
    "                    # does not depend on them (we add them in below)\n",
    "                    #print(next_tag, forward_var.shape, self.transitions.shape, self.transitions[next_tag].shape )\n",
    "                    next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                    best_tag_id = argmax(next_tag_var)\n",
    "                    bptrs_t.append(best_tag_id)\n",
    "                    viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "                # Now add in the emission scores, and assign forward_var to the set\n",
    "                # of viterbi variables we just computed\n",
    "                forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "                backpointers.append(bptrs_t)\n",
    "\n",
    "            # Transition to STOP_TAG\n",
    "            terminal_var = forward_var  #+ self.transitions[self.tag_to_ix[STOP_TAG]] #+ end_vvars\n",
    "            best_tag_id = argmax(terminal_var)\n",
    "            path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "            # Follow the back pointers to decode the best path.\n",
    "            best_path = [best_tag_id]\n",
    "            for bptrs_t in reversed(backpointers):\n",
    "                best_tag_id = bptrs_t[best_tag_id]\n",
    "                best_path.append(best_tag_id)\n",
    "\n",
    "            best_path.reverse()\n",
    "            all_scores.append(path_score)\n",
    "            all_bestpath.append(best_path)\n",
    "            \n",
    "        return all_scores, all_bestpath        \n",
    "\n",
    "    def neg_log_likelihood(self, x, t, y):\n",
    "        feats = self._get_lstm_features(x, t)\n",
    "        forward_score = self._forward_alg(feats[0])\n",
    "        gold_score = self._score_sentence(feats[0], y[0])\n",
    "        return forward_score - gold_score\n",
    "\n",
    "      \n",
    "   \n",
    "    def _get_lstm_features(self, x, t):\n",
    "        word_embed = self.word_embed(x)\n",
    "        tg_embed = self.tg_embed(t)\n",
    "        \n",
    "        embed = torch.cat((word_embed,tg_embed),-1)\n",
    "        output, hidden = self.rnn(embed)\n",
    "\n",
    "        frame_scores = self.decision_frame(output)\n",
    "        return frame_scores\n",
    "   \n",
    "      \n",
    "    def forward(self, x, t):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        frame_scores = self._get_lstm_features(x, t)      \n",
    "        \n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(frame_scores)\n",
    "        \n",
    "        return score, tag_seq\n",
    "      \n",
    "      \n",
    "rnn_crf_model = RNN_CRF()\n",
    "rnn_crf_model\n",
    "\n",
    "a, b = rnn_crf_model(Variable(X[:2]), Variable(T[:2]))\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDMVSf29Un76"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-efb671efea27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfit_crf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_crf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-efb671efea27>\u001b[0m in \u001b[0;36mfit_crf\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def fit_crf(model, epochs):\n",
    "  \n",
    "    optimizer = optim.Adam(filter(lambda param: param.requires_grad, model.parameters()))\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = num = 0\n",
    "        for x, t, p, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.neg_log_likelihood(x, t, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            num += len(y)\n",
    "        print(epoch, total_loss / num )\n",
    "\n",
    "fit_crf(rnn_crf_model, 20)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Frame_Semantic_Tagger.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
