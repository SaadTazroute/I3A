{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "sLao8QcyC9lE",
    "outputId": "3393a81a-48d4-4db2-8237-ca5797e0f0ea"
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "-gBjsR_yBRKE",
    "outputId": "3c5ee8c2-a368-4a32-ac08-5df6b26b87a3"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "! wget http://mattmahoney.net/dc/text8.zip -P /tmp\n",
    "! unzip /tmp/text8.zip -d /tmp/\n",
    "! head -c10000000 /tmp/text8 > /tmp/text8_10mo\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0F-CPnwLUaQ"
   },
   "outputs": [],
   "source": [
    "# lire les donnés que nous avons téléchargé\n",
    "sentences = word2vec.Text8Corpus('/tmp/text8_10mo')\n",
    "for s in sentences:\n",
    "  print(\"a sentence\",s[0:20])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7-63V6eu_PJ"
   },
   "outputs": [],
   "source": [
    "# Apprendre un modèle de embeddings sur text8\n",
    "params = {\n",
    "    'alpha': 0.05,   # learning rate\n",
    "    'size':  100,    # number of dimensions for the dense representations\n",
    "    'window': 5,     # context window size\n",
    "    'iter':   5,     # nb of iterations \n",
    "    'min_count': 5,  # to ignore very rare words\n",
    "    'negative': 5    # we need negative examples, how many?\n",
    "}\n",
    "\n",
    "my_model = word2vec.Word2Vec(sentences, **params)\n",
    "print(my_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4RgGrOLD3L1"
   },
   "outputs": [],
   "source": [
    "word = 'services'\n",
    "most_similar = my_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('word',word,'most_similar: ', most_similar)\n",
    "\n",
    "word = 'japanese'\n",
    "most_similar = my_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('word',word,'most_similar: ', most_similar)\n",
    "\n",
    "word = 'six'\n",
    "most_similar = my_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('word',word,'most_similar: ', most_similar)\n",
    "\n",
    "word = 'good'\n",
    "most_similar = my_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('word',word,'most_similar: ', most_similar)\n",
    "\n",
    "\n",
    "# Calcul de la similarité cosinus entre deux mots\n",
    "# cos_sim( a,b) =  a*b/|a||b|\n",
    "cos_sim = np.dot( my_model.wv['services'],my_model.wv['facilities'] )/(np.linalg.norm(my_model.wv['services'], ord=2)*np.linalg.norm(my_model.wv['facilities'], ord=2))\n",
    "print('consine similarity between', 'services' , 'facilities', cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tVvSWqmEYVj"
   },
   "outputs": [],
   "source": [
    "# Relations semantiques dans les embeddings\n",
    "most_similar = my_model.wv.most_similar(positive=['woman', 'husband'], negative=['man'], topn=5)\n",
    "print('most_similar: ', most_similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jtk1_sZsj1eQ"
   },
   "outputs": [],
   "source": [
    "most_similar = my_model.wv.most_similar(negative=['korean'], topn=5)\n",
    "print('most_similar: ', most_similar)\n",
    "\n",
    "\n",
    "most_similar = my_model.wv.most_similar(positive=['korean'], topn=10000000000)\n",
    "print('most_similar: ', most_similar[-5:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFONMsDMO-Br"
   },
   "source": [
    "### How to select your parameters?\n",
    "\n",
    "Les deux parametres les plus importants sont la taille de la fenêtre et le nombre minimal de occurrences\n",
    "\n",
    "La taille de fenêtre a un impact direct sur ce qui est modelisé :\n",
    "\n",
    "- Des petites fenetres donnent des embeddings très syntaxiques (similarity)\n",
    "\n",
    "- Des fenetres plus grandes donnent des embeddings plus sémantiques (relatedness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_HAqEMYL65B"
   },
   "outputs": [],
   "source": [
    "# Importance de la taille de fenetre\n",
    "params = {\n",
    "    'alpha': 0.05,   # learning rate\n",
    "    'size':  100,    # number of dimensions for the dense representations\n",
    "    'window': 15,     # context window size\n",
    "    'iter':   5,     # nb of iterations \n",
    "    'min_count': 5,  # to ignore very rare words\n",
    "    'negative': 5    # we need negative examples, how many?\n",
    "}\n",
    "\n",
    "largecontext_model = word2vec.Word2Vec(sentences, **params)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'alpha': 0.05,   # learning rate\n",
    "    'size':  100,    # number of dimensions for the dense representations\n",
    "    'window': 2,     # context window size\n",
    "    'iter':   5,     # nb of iterations \n",
    "    'min_count': 5,  # to ignore very rare words\n",
    "    'negative': 5    # we need negative examples, how many?\n",
    "}\n",
    "\n",
    "smallcontext_model = word2vec.Word2Vec(sentences, **params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gE8M30vaMSN9"
   },
   "outputs": [],
   "source": [
    "word = 'food'\n",
    "most_similar = largecontext_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('Using a large context... word',word,'most_similar: ', most_similar)\n",
    "\n",
    "most_similar = smallcontext_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('Using a small context... word',word,'most_similar: ', most_similar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word = 'english'\n",
    "most_similar = largecontext_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('Using a large context... word',word,'most_similar: ', most_similar)\n",
    "\n",
    "most_similar = smallcontext_model.wv.most_similar(positive=[word], topn=5)\n",
    "print('Using a small context... word',word,'most_similar: ', most_similar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25msJnShQGya"
   },
   "source": [
    "### Stabilité des Embeddings\n",
    "\n",
    "L'information utile dans les embeddings est encodé dans les similarités entre les mots et non pas dans leur position dans l'espace\n",
    "\n",
    "Plusieurs executions de W2V sur les mêmes données produissent des embeddings très similaires.\n",
    "\n",
    "C'est-à-dire, les plus proche voisins de chaque mot ne changent presque pas.\n",
    "\n",
    "Mais malheureusement, deux modèles d'embeddings ne partagent pas le même espace de representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkFE-nB1QxZL"
   },
   "outputs": [],
   "source": [
    "# Nous allons apprendre deux embeddings avec les même parametres\n",
    "params = {\n",
    "    'alpha': 0.05,   # learning rate\n",
    "    'size':  100,    # number of dimensions for the dense representations\n",
    "    'window': 5,     # context window size\n",
    "    'iter':   5,     # nb of iterations \n",
    "    'min_count': 5,  # to ignore very rare words\n",
    "    'negative': 5    # we need negative examples, how many?\n",
    "}\n",
    "\n",
    "model_run1 = word2vec.Word2Vec(sentences, **params)\n",
    "model_run2 = word2vec.Word2Vec(sentences, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwPDXWaYRhK0"
   },
   "outputs": [],
   "source": [
    "# Nous devrions trouver a peu près les mêmes voisins pour tous les mots\n",
    "word = 'france'\n",
    "most_similar_run1 = model_run1.wv.most_similar(positive=[word], topn=10)\n",
    "most_similar_run2 = model_run2.wv.most_similar(positive=[word], topn=10)\n",
    "for r1,r2 in zip(most_similar_run1,most_similar_run2):\n",
    "  print('run1',r1,'run2',r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3liERQvSMqe"
   },
   "outputs": [],
   "source": [
    "# Les similarités sont les mêmes, mais malheuresement les coordonées sont très differents \n",
    "\n",
    "word = 'france'\n",
    "print(model_run1.wv[word])\n",
    "print(model_run2.wv[word])\n",
    "\n",
    "# We can compute the cosine similarity across models\n",
    "# If the models were compatible the similarity ~1 for every word.\n",
    "cos_sim = np.dot( model_run1.wv[word],model_run2.wv[word] )/(np.linalg.norm(model_run1.wv[word], ord=2)*np.linalg.norm(model_run2.wv[word], ord=2))\n",
    "print('consine similarity across models for word:', word , 'is', cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pXUNDnKTuI_"
   },
   "source": [
    "### Alignement des embeddings après apprentissage\n",
    "\n",
    "Voici comment aligner des embeddigs dans un même espace.\n",
    "\n",
    "Il y a plusieurs methodes. La plus simple consiste a utiliser un dictionnaire de mots de reference a aligner.\n",
    "\n",
    "Pour construire une la transformation lineaire qui nous envoie d'un espace vers l'autre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3W27ulvDcf5Y"
   },
   "outputs": [],
   "source": [
    "# try different values of the number of pivots. The more words used during the alignment the better\n",
    "nb_pivots = 500\n",
    "wordlist = list(model_run1.wv.vocab.keys())[0:nb_pivots]\n",
    "model_run1.init_sims()\n",
    "model_run2.init_sims()\n",
    "\n",
    "\n",
    "# Extraire le vocabulaire de chaque model\n",
    "vocab_m1 = set(model_run1.wv.vocab.keys())\n",
    "vocab_m2 = set(model_run2.wv.vocab.keys())\n",
    "\n",
    "# Extraire le vocabulaire en commun avec la liste \n",
    "common_vocab = list(vocab_m1&vocab_m2&set(wordlist))\n",
    "\n",
    "syn0norms = []\n",
    "# pour chaque modèle...\n",
    "for m in [model_run1,model_run2]:\n",
    "\t# Remplacer syn0norm avec un nouveau array uniquement avec le mots en commun\n",
    "\tindices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "\told_arr = m.wv.syn0norm\n",
    "\tnew_arr = np.array([old_arr[index] for index in indices])\n",
    "\tsyn0norms.append( new_arr )\n",
    "  \n",
    "base_vecs = syn0norms[0]\n",
    "other_vecs = syn0norms[1]\n",
    "\n",
    "# produit scalaire entre les vecteurs de chaque espace\n",
    "m = other_vecs.T.dot(base_vecs) \n",
    "\n",
    "# SVD method \n",
    "u, _, v = np.linalg.svd(m)\n",
    "ortho = u.dot(v) \n",
    "\n",
    "# Replacer les embeddings avec les projections\n",
    "model_run2.wv.syn0norm = model_run2.wv.syn0 = (model_run2.wv.syn0norm).dot(ortho)\n",
    "\n",
    "\n",
    "word = 'france'\n",
    "print(model_run1.wv[word])\n",
    "print(model_run2.wv[word])\n",
    "\n",
    "# We can compute the cosine similarity across models\n",
    "# If the models were compatible the similarity ~1 for every word.\n",
    "cos_sim = np.dot( model_run1.wv[word],model_run2.wv[word] )/(np.linalg.norm(model_run1.wv[word], ord=2)*np.linalg.norm(model_run2.wv[word], ord=2))\n",
    "print('consine similarity across models for word:', word , 'is', cos_sim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FaROExcpodd"
   },
   "source": [
    "### Mots hors vocabulaire\n",
    "\n",
    "Je peux apprendre un embedding pour les mots inconnues avec word2vec. \n",
    "\n",
    "Par exemple, je peux reemplace tous les mots rares (count<5) par un token \"_oov_\"  et apprendre mes embeddings normalement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gk6D1clVndYj"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "oov_th = 1\n",
    "\n",
    "count = Counter([x for sent in sentences for x in sent])\n",
    "oov_sentences = []\n",
    "for sent in sentences:\n",
    "  oov_sentences.append( [ s if(count[s]>oov_th) else '_oov_' for s in sent ] )\n",
    "\n",
    "  \n",
    "for oov_sent,sent in zip(list(oov_sentences)[0:5], list(sentences)[0:5]):\n",
    "  print(oov_sent[0:20])\n",
    "  print(sent[0:20])\n",
    "  \n",
    "\n",
    "params = {\n",
    "    'alpha': 0.05,   # learning rate\n",
    "    'size':  100,    # number of dimensions for the dense representations\n",
    "    'window': 15,     # context window size\n",
    "    'iter':   5,     # nb of iterations \n",
    "    'min_count': oov_th,  # to ignore very rare words\n",
    "    'negative': 5    # we need negative examples, how many?\n",
    "}\n",
    "\n",
    "oov_model = word2vec.Word2Vec(oov_sentences, **params)\n",
    "print( 'oov_embedding', oov_model.wv['_oov_'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTcaM2uL7c6U"
   },
   "outputs": [],
   "source": [
    "most_similar = oov_model.wv.most_similar(positive=['_oov_'], topn=10)\n",
    "print(' word',word,'most_similar: ', most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYGptOLh1-p4"
   },
   "outputs": [],
   "source": [
    "word = 'france'\n",
    "try:\n",
    "    oov_model[word]\n",
    "    print('found a vector for word', word)\n",
    "except:\n",
    "    print('there is no vector for word', word)\n",
    "\n",
    "    \n",
    "word = 'francee'\n",
    "try:\n",
    "    oov_model[word]\n",
    "    print('found a vector for word', word)\n",
    "except:\n",
    "    print('there is no vector for word', word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Do6LdTv-GbTD"
   },
   "source": [
    "### Handling Unknown words due to typing errors\n",
    "\n",
    "Sometimes we have unknown words produced due to typing errors.\n",
    "\n",
    "The technique above mentioned is not well suited for this type of errors.\n",
    "\n",
    "However, there are other ways to generate embeddings that make systems more robuts to oov words.\n",
    "\n",
    "Fasttext is intended to solve this kind of problems. Lets learn how to use it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Gs5T6G60tXC"
   },
   "outputs": [],
   "source": [
    "!pip install cython \n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1T83PHU1gYm"
   },
   "outputs": [],
   "source": [
    "# more info about fasttext https://pypi.org/project/fasttext/\n",
    "import fasttext\n",
    "model = fasttext.cbow( '/tmp/text8_10mo', output=\"/tmp/fasttext\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UipHffE2u-Q"
   },
   "outputs": [],
   "source": [
    "# fasttext builds word embeddings based on the character ngrams in the word\n",
    "# france  = <^fr> <fra> <ran> <anc> <nce> <ce$>       <^fran> <franc> <rance> <ance$>\n",
    "# francee = <^fr> <fra> <ran> <anc> <nce> <cee> <ee$> <^fran> <franc> <rance> <ancee> <ncee$> \n",
    "word = 'france'\n",
    "vector_normal = model['france']\n",
    "vector_faute = model['francee']\n",
    "\n",
    "cos_sim = np.dot(vector_normal,vector_faute)/(np.linalg.norm(vector_normal, ord=2)*np.linalg.norm(vector_faute, ord=2))\n",
    "print('consine similarity in fasttext with errors for word:', word , 'is', cos_sim)\n",
    "\n",
    "\n",
    "# errors in the middle of the word affect more ngrams\n",
    "word = 'france'\n",
    "vector_normal = model['france']\n",
    "vector_faute = model['fraance']\n",
    "\n",
    "cos_sim = np.dot(vector_normal,vector_faute)/(np.linalg.norm(vector_normal, ord=2)*np.linalg.norm(vector_faute, ord=2))\n",
    "print('consine similarity in fasttext with errors for word:', word , 'is', cos_sim)\n",
    "\n",
    "\n",
    "# if you swap triplets you get very similar embeddings\n",
    "word = \"anticonstitutionellement\"\n",
    "vector_normal = model['anticonstitutionellement']\n",
    "vector_faute = model['antitionnelleconstitument']\n",
    "\n",
    "cos_sim = np.dot(vector_normal,vector_faute)/(np.linalg.norm(vector_normal, ord=2)*np.linalg.norm(vector_faute, ord=2))\n",
    "print('consine similarity in fasttext with errors for word:', word , 'is', cos_sim)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings_101.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
